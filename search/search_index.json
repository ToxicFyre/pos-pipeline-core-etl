{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"POS Core ETL","text":"<p>A comprehensive Python package for Point of Sale (POS) data processing, forecasting, and quality assurance.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>ETL Pipeline: Extract, transform, and load POS payment and sales data</li> <li>Time Series Forecasting: Generate ARIMA-based forecasts for payment metrics</li> <li>Quality Assurance: Automated data validation and anomaly detection</li> <li>Multi-Branch Support: Handle multiple sucursales (branches) with code window tracking</li> <li>Incremental Processing: Smart date range chunking and existing data discovery</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from pathlib import Path\nfrom pos_core.etl import PaymentsETLConfig, get_payments, get_payments_forecast\n\n# Configure and get payments data\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\npayments = get_payments(\"2025-01-01\", \"2025-01-31\", config)\n\n# Generate forecast\nforecast = get_payments_forecast(\"2025-01-31\", horizon_weeks=1, config=config)\nprint(forecast.head())\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pos-core-etl\n</code></pre> <p>For development:</p> <pre><code>pip install -e .[dev]\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>User Guide - Installation, configuration, and usage</li> <li>API Reference - Complete API documentation</li> <li>Examples - Runnable example scripts</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"dev-notes/","title":"Development Notes - ETL API Refactor","text":""},{"location":"dev-notes/#target-api","title":"Target API","text":"<p>The goal of this refactor is to create a clean, lean API that matches how we think about ETL workflows.</p>"},{"location":"dev-notes/#etl-configs","title":"ETL Configs","text":"<ul> <li><code>SalesETLConfig</code></li> <li><code>PaymentsETLConfig</code></li> </ul>"},{"location":"dev-notes/#stage-commands","title":"Stage Commands","text":"<ul> <li><code>download_sales(...)</code>, <code>clean_sales(...)</code>, <code>aggregate_sales(...)</code></li> <li><code>download_payments(...)</code>, <code>clean_payments(...)</code>, <code>aggregate_payments(...)</code></li> </ul>"},{"location":"dev-notes/#query-pandas-creator","title":"Query / Pandas Creator","text":"<ul> <li><code>get_sales(..., level=\"ticket|group|day\", refresh=False)</code></li> <li><code>get_payments(..., refresh=False)</code></li> <li><code>get_payments_forecast(..., horizon_weeks=13, refresh=False)</code></li> </ul> <p>Everything we build should support this set of calls.</p>"},{"location":"dev-notes/#constraints","title":"Constraints","text":"<p>To keep the codebase lean and prevent overbuilding:</p> <ol> <li>No new public function unless I have a real use for it right now.</li> <li>One concept \u2192 one way to do it.</li> <li>Public API uses simple types: strings, lists, DataFrames, dataclasses.</li> <li>If it feels too clever, don't do it.</li> </ol> <p>Check new code against this list before adding it.</p>"},{"location":"dev-notes/#current-public-api-inventory","title":"Current Public API Inventory","text":""},{"location":"dev-notes/#from-pos_coreetl","title":"From <code>pos_core.etl</code>","text":"<ul> <li><code>PaymentsETLConfig</code>, <code>PaymentsPaths</code></li> <li><code>build_payments_dataset</code></li> </ul>"},{"location":"dev-notes/#from-pos_coreforecasting","title":"From <code>pos_core.forecasting</code>","text":"<ul> <li><code>ForecastConfig</code>, <code>ForecastResult</code></li> <li><code>run_payments_forecast</code></li> </ul>"},{"location":"dev-notes/#from-pos_coreqa","title":"From <code>pos_core.qa</code>","text":"<ul> <li><code>PaymentsQAResult</code></li> <li><code>run_payments_qa</code></li> </ul>"},{"location":"dev-notes/#internal-modules","title":"Internal Modules","text":"<p>These are treated as internal (not re-exported from <code>__init__</code>): - <code>pos_core.etl.a_extract.*</code> - <code>pos_core.etl.b_transform.*</code> - <code>pos_core.etl.c_load.*</code> - Helper modules: <code>pos_core.etl.utils</code>, <code>pos_core.etl.branch_config</code></p>"},{"location":"dev-notes/#api-decisions","title":"API Decisions","text":""},{"location":"dev-notes/#keep-as-is","title":"Keep as-is","text":"<ul> <li><code>build_payments_dataset</code> - Will wrap internally later using stage functions</li> </ul>"},{"location":"dev-notes/#wrap","title":"Wrap","text":"<ul> <li>Will create <code>download_payments</code>, <code>clean_payments</code>, <code>aggregate_payments</code> that call existing internals</li> <li>Will create <code>download_sales</code>, <code>clean_sales</code>, <code>aggregate_sales</code> that call existing internals</li> </ul>"},{"location":"dev-notes/#mark-as-internal","title":"Mark as internal","text":"<ul> <li>All functions in <code>a_extract</code>, <code>b_transform</code>, <code>c_load</code> are already internal (not exported)</li> </ul>"},{"location":"dev-notes/#periodic-lean-audit","title":"Periodic Lean Audit","text":"<p>Every few weeks, ask:</p> <ol> <li>Is there any public function nobody is using?</li> <li>Is there any duplicated logic between stage and query functions?</li> <li>Could two similar functions be merged into one with a simple parameter?</li> </ol>"},{"location":"api-reference/etl/","title":"ETL API Reference","text":""},{"location":"api-reference/etl/#configuration","title":"Configuration","text":""},{"location":"api-reference/etl/#paymentsetlconfig","title":"<code>PaymentsETLConfig</code>","text":"<p>Configuration dataclass for payments ETL pipeline.</p>"},{"location":"api-reference/etl/#attributes","title":"Attributes","text":"<ul> <li><code>paths</code> (PaymentsPaths): All filesystem paths used by the pipeline</li> <li><code>chunk_size_days</code> (int): Maximum number of days per HTTP request chunk (default: 180)</li> <li><code>excluded_branches</code> (List[str]): List of branch names to exclude from processing (default: [\"CEDIS\"])</li> </ul>"},{"location":"api-reference/etl/#methods","title":"Methods","text":""},{"location":"api-reference/etl/#from_data_root","title":"<code>from_data_root()</code>","text":"<p>Build a default config given a data_root.</p> <pre><code>config = PaymentsETLConfig.from_data_root(\n    data_root=Path(\"data\"),\n    sucursales_json=Path(\"utils/sucursales.json\"),\n    chunk_size_days=180\n)\n</code></pre>"},{"location":"api-reference/etl/#from_root","title":"<code>from_root()</code>","text":"<p>Alias for <code>from_data_root()</code> for consistency with <code>SalesETLConfig</code>.</p> <pre><code>config = PaymentsETLConfig.from_root(\n    data_root=Path(\"data\"),\n    sucursales_file=Path(\"utils/sucursales.json\"),\n    chunk_size_days=180\n)\n</code></pre>"},{"location":"api-reference/etl/#paymentspaths","title":"<code>PaymentsPaths</code>","text":"<p>Path configuration for ETL stages.</p>"},{"location":"api-reference/etl/#attributes_1","title":"Attributes","text":"<ul> <li><code>raw_payments</code> (Path): Directory for raw payment Excel files</li> <li><code>clean_payments</code> (Path): Directory for cleaned payment CSV files</li> <li><code>proc_payments</code> (Path): Directory for processed/aggregated payment data</li> <li><code>sucursales_json</code> (Path): Path to sucursales.json configuration file</li> </ul>"},{"location":"api-reference/etl/#salesetlconfig","title":"<code>SalesETLConfig</code>","text":"<p>Configuration dataclass for sales ETL pipeline.</p>"},{"location":"api-reference/etl/#attributes_2","title":"Attributes","text":"<ul> <li><code>paths</code> (SalesPaths): All filesystem paths used by the pipeline</li> <li><code>chunk_days</code> (int): Maximum number of days per HTTP request chunk (default: 180)</li> </ul>"},{"location":"api-reference/etl/#methods_1","title":"Methods","text":""},{"location":"api-reference/etl/#from_root_1","title":"<code>from_root()</code>","text":"<p>Build a default config given a data_root and sucursales file.</p> <pre><code>config = SalesETLConfig.from_root(\n    data_root=Path(\"data\"),\n    sucursales_file=Path(\"utils/sucursales.json\")\n)\n</code></pre>"},{"location":"api-reference/etl/#salespaths","title":"<code>SalesPaths</code>","text":"<p>Path configuration for sales ETL stages.</p>"},{"location":"api-reference/etl/#attributes_3","title":"Attributes","text":"<ul> <li><code>raw_sales</code> (Path): Directory for raw sales Excel files</li> <li><code>clean_sales</code> (Path): Directory for cleaned sales CSV files</li> <li><code>proc_sales</code> (Path): Directory for processed/aggregated sales data</li> <li><code>sucursales_json</code> (Path): Path to sucursales.json configuration file</li> </ul>"},{"location":"api-reference/etl/#query-functions","title":"Query Functions","text":"<p>Query functions are the recommended way to get data. They automatically run ETL stages only when needed based on metadata.</p>"},{"location":"api-reference/etl/#get_payments","title":"<code>get_payments()</code>","text":"<p>Get payments data, running stages only if needed.</p>"},{"location":"api-reference/etl/#signature","title":"Signature","text":"<pre><code>def get_payments(\n    start_date: str,\n    end_date: str,\n    config: PaymentsETLConfig,\n    branches: Optional[List[str]] = None,\n    refresh: bool = False,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#parameters","title":"Parameters","text":"<ul> <li><code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive)</li> <li><code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive)</li> <li><code>config</code> (PaymentsETLConfig): Configuration instance</li> <li><code>branches</code> (Optional[List[str]]): List of branch names to process. If None, processes all branches.</li> <li><code>refresh</code> (bool): If True, force re-run all stages. If False, check metadata and skip completed stages.</li> </ul>"},{"location":"api-reference/etl/#returns","title":"Returns","text":"<p>DataFrame containing aggregated payments data (one row per sucursal + fecha).</p>"},{"location":"api-reference/etl/#example","title":"Example","text":"<pre><code>from pos_core.etl import PaymentsETLConfig, get_payments\n\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\ndf = get_payments(\"2025-01-01\", \"2025-01-31\", config, refresh=False)\n</code></pre>"},{"location":"api-reference/etl/#get_sales","title":"<code>get_sales()</code>","text":"<p>Get sales data at the specified level, running stages only if needed.</p>"},{"location":"api-reference/etl/#signature_1","title":"Signature","text":"<pre><code>def get_sales(\n    start_date: str,\n    end_date: str,\n    config: SalesETLConfig,\n    branches: Optional[List[str]] = None,\n    level: str = \"ticket\",  # \"ticket\" | \"group\" | \"day\"\n    refresh: bool = False,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#parameters_1","title":"Parameters","text":"<ul> <li><code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive)</li> <li><code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive)</li> <li><code>config</code> (SalesETLConfig): Configuration instance</li> <li><code>branches</code> (Optional[List[str]]): List of branch names to process. If None, processes all branches.</li> <li><code>level</code> (str): Aggregation level: \"ticket\", \"group\", or \"day\" (default: \"ticket\")</li> <li><code>refresh</code> (bool): If True, force re-run all stages. If False, check metadata and skip completed stages.</li> </ul>"},{"location":"api-reference/etl/#returns_1","title":"Returns","text":"<p>DataFrame containing aggregated sales data at the specified level.</p>"},{"location":"api-reference/etl/#example_1","title":"Example","text":"<pre><code>from pos_core.etl import SalesETLConfig, get_sales\n\nconfig = SalesETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\ndf_ticket = get_sales(\"2025-01-01\", \"2025-01-31\", config, level=\"ticket\")\ndf_group = get_sales(\"2025-01-01\", \"2025-01-31\", config, level=\"group\")\n</code></pre>"},{"location":"api-reference/etl/#get_payments_forecast","title":"<code>get_payments_forecast()</code>","text":"<p>Get payments forecast for the specified horizon.</p>"},{"location":"api-reference/etl/#signature_2","title":"Signature","text":"<pre><code>def get_payments_forecast(\n    as_of: str,  # Date string\n    horizon_weeks: int,\n    config: PaymentsETLConfig,\n    refresh: bool = False,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#parameters_2","title":"Parameters","text":"<ul> <li><code>as_of</code> (str): Date string in YYYY-MM-DD format (forecast as of this date)</li> <li><code>horizon_weeks</code> (int): Number of weeks to forecast ahead</li> <li><code>config</code> (PaymentsETLConfig): Configuration instance</li> <li><code>refresh</code> (bool): If True, force re-run ETL stages before forecasting. If False, use existing data if available.</li> </ul>"},{"location":"api-reference/etl/#returns_2","title":"Returns","text":"<p>DataFrame containing forecast results with columns: <code>sucursal</code>, <code>fecha</code>, <code>metric</code>, <code>valor</code>.</p>"},{"location":"api-reference/etl/#example_2","title":"Example","text":"<pre><code>from pos_core.etl import PaymentsETLConfig, get_payments_forecast\n\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\nforecast = get_payments_forecast(\"2025-11-24\", horizon_weeks=13, config=config)\n</code></pre>"},{"location":"api-reference/etl/#stage-functions","title":"Stage Functions","text":"<p>Stage functions provide fine-grained control over individual ETL stages. They are used internally by query functions and can be called directly for advanced use cases.</p>"},{"location":"api-reference/etl/#payments-stage-functions","title":"Payments Stage Functions","text":""},{"location":"api-reference/etl/#download_payments","title":"<code>download_payments()</code>","text":"<p>Download raw payments Excel for the given range.</p> <pre><code>def download_payments(\n    start_date: str,\n    end_date: str,\n    config: PaymentsETLConfig,\n    branches: Optional[List[str]] = None,\n    force: bool = False,\n) -&gt; None\n</code></pre>"},{"location":"api-reference/etl/#clean_payments","title":"<code>clean_payments()</code>","text":"<p>Transform raw payments files into clean CSV/Parquet.</p> <pre><code>def clean_payments(\n    start_date: str,\n    end_date: str,\n    config: PaymentsETLConfig,\n    branches: Optional[List[str]] = None,\n    force: bool = False,\n) -&gt; None\n</code></pre>"},{"location":"api-reference/etl/#aggregate_payments","title":"<code>aggregate_payments()</code>","text":"<p>Aggregate clean payments into the canonical dataset and return it.</p> <pre><code>def aggregate_payments(\n    start_date: str,\n    end_date: str,\n    config: PaymentsETLConfig,\n    branches: Optional[List[str]] = None,\n    force: bool = False,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#sales-stage-functions","title":"Sales Stage Functions","text":""},{"location":"api-reference/etl/#download_sales","title":"<code>download_sales()</code>","text":"<p>Download raw sales Excel for the given range.</p> <pre><code>def download_sales(\n    start_date: str,\n    end_date: str,\n    config: SalesETLConfig,\n    branches: Optional[List[str]] = None,\n    force: bool = False,\n) -&gt; None\n</code></pre>"},{"location":"api-reference/etl/#clean_sales","title":"<code>clean_sales()</code>","text":"<p>Transform raw sales files into clean CSV.</p> <pre><code>def clean_sales(\n    start_date: str,\n    end_date: str,\n    config: SalesETLConfig,\n    branches: Optional[List[str]] = None,\n    force: bool = False,\n) -&gt; None\n</code></pre>"},{"location":"api-reference/etl/#aggregate_sales","title":"<code>aggregate_sales()</code>","text":"<p>Aggregate clean sales at the specified level.</p> <pre><code>def aggregate_sales(\n    start_date: str,\n    end_date: str,\n    config: SalesETLConfig,\n    level: str = \"ticket\",  # \"ticket\" | \"group\" | \"day\"\n    branches: Optional[List[str]] = None,\n    force: bool = False,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#orchestration-functions","title":"Orchestration Functions","text":""},{"location":"api-reference/etl/#build_payments_dataset","title":"<code>build_payments_dataset()</code>","text":"<p>Main orchestration function for payments ETL. This is a thin wrapper around the stage functions.</p>"},{"location":"api-reference/etl/#signature_3","title":"Signature","text":"<pre><code>def build_payments_dataset(\n    start_date: str,\n    end_date: str,\n    config: PaymentsETLConfig,\n    branches: Optional[List[str]] = None,\n    steps: Optional[List[str]] = None,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#parameters_3","title":"Parameters","text":"<ul> <li><code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive)</li> <li><code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive)</li> <li><code>config</code> (PaymentsETLConfig): Configuration instance</li> <li><code>branches</code> (Optional[List[str]]): List of branch names to process. If None, processes all branches.</li> <li><code>steps</code> (Optional[List[str]]): List of steps to execute. Valid steps: \"extract\", \"transform\", \"aggregate\". If None, executes all steps.</li> </ul>"},{"location":"api-reference/etl/#returns_3","title":"Returns","text":"<p>DataFrame containing the aggregated payments data (one row per sucursal + fecha).</p>"},{"location":"api-reference/etl/#raises","title":"Raises","text":"<ul> <li><code>ConfigError</code>: If invalid step names are provided</li> <li><code>FileNotFoundError</code>: If the aggregated file is expected but missing</li> </ul>"},{"location":"api-reference/etl/#example_3","title":"Example","text":"<pre><code>from pathlib import Path\nfrom pos_core.etl import PaymentsETLConfig, build_payments_dataset\n\nconfig = PaymentsETLConfig.from_data_root(Path(\"data\"))\ndf = build_payments_dataset(\"2023-01-01\", \"2023-12-31\", config)\n</code></pre> <p>Note: For most use cases, <code>get_payments()</code> is recommended as it provides automatic idempotence through metadata checks.</p>"},{"location":"api-reference/exceptions/","title":"Exceptions API Reference","text":"<p>The package defines custom exceptions that are part of the public API. All exceptions inherit from <code>PosAPIError</code> for easy catching.</p>"},{"location":"api-reference/exceptions/#posapierror","title":"<code>PosAPIError</code>","text":"<p>Base exception for all POS Core ETL errors.</p> <p>This is the base class for all domain-specific exceptions in the package. Users can catch this exception to handle any POS Core ETL error.</p> <pre><code>from pos_core.exceptions import PosAPIError\n\ntry:\n    # Some POS Core ETL operation\n    pass\nexcept PosAPIError as e:\n    # Handle any POS Core ETL error\n    print(f\"POS Core ETL error: {e}\")\n</code></pre>"},{"location":"api-reference/exceptions/#configerror","title":"<code>ConfigError</code>","text":"<p>Raised when there is a configuration error.</p> <p>This exception is raised when: - Invalid configuration values are provided - Required configuration is missing - Configuration files cannot be loaded or parsed</p> <pre><code>from pos_core.exceptions import ConfigError\n\ntry:\n    # Configuration operation\n    pass\nexcept ConfigError as e:\n    # Handle configuration error\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"api-reference/exceptions/#dataqualityerror","title":"<code>DataQualityError</code>","text":"<p>Raised when data quality checks fail.</p> <p>This exception is raised when: - Required columns are missing from input data - Data validation fails - Data quality checks detect critical issues</p> <pre><code>from pos_core.exceptions import DataQualityError\n\ntry:\n    # Data quality operation\n    pass\nexcept DataQualityError as e:\n    # Handle data quality error\n    print(f\"Data quality error: {e}\")\n</code></pre>"},{"location":"api-reference/forecasting/","title":"Forecasting API Reference","text":""},{"location":"api-reference/forecasting/#forecastconfig","title":"<code>ForecastConfig</code>","text":"<p>Configuration for payments forecasting.</p>"},{"location":"api-reference/forecasting/#attributes","title":"Attributes","text":"<ul> <li><code>horizon_days</code> (int): Number of days ahead to forecast (default: 7)</li> <li><code>metrics</code> (List[str]): List of metrics to forecast (default: cash, credit, debit, total)</li> <li><code>branches</code> (Optional[List[str]]): List of branch names to forecast. If None, infers from payments_df.</li> </ul>"},{"location":"api-reference/forecasting/#example","title":"Example","text":"<pre><code>from pos_core.forecasting import ForecastConfig\n\nconfig = ForecastConfig(\n    horizon_days=14,\n    metrics=[\"ingreso_efectivo\", \"ingreso_total\"],\n    branches=[\"Banana\", \"Queen\"]\n)\n</code></pre>"},{"location":"api-reference/forecasting/#forecastresult","title":"<code>ForecastResult</code>","text":"<p>Result dataclass containing forecast DataFrame, deposit schedule, and metadata.</p>"},{"location":"api-reference/forecasting/#attributes_1","title":"Attributes","text":"<ul> <li><code>forecast</code> (pd.DataFrame): DataFrame with columns: sucursal, fecha, metric, valor</li> <li><code>deposit_schedule</code> (pd.DataFrame): DataFrame with cash-flow deposit schedule</li> <li><code>metadata</code> (Dict[str, object]): Dictionary with additional metadata</li> </ul>"},{"location":"api-reference/forecasting/#metadata-keys","title":"Metadata Keys","text":"<ul> <li><code>branches</code>: List of branches forecasted</li> <li><code>metrics</code>: List of metrics forecasted</li> <li><code>horizon_days</code>: Number of days forecasted</li> <li><code>last_historical_date</code>: Last date in historical data</li> <li><code>successful_forecasts</code>: Number of successful forecasts</li> <li><code>failed_forecasts</code>: Number of failed forecasts</li> </ul>"},{"location":"api-reference/forecasting/#run_payments_forecast","title":"<code>run_payments_forecast()</code>","text":"<p>Main forecasting function.</p>"},{"location":"api-reference/forecasting/#signature","title":"Signature","text":"<pre><code>def run_payments_forecast(\n    payments_df: pd.DataFrame,\n    config: Optional[ForecastConfig] = None,\n) -&gt; ForecastResult\n</code></pre>"},{"location":"api-reference/forecasting/#parameters","title":"Parameters","text":"<ul> <li><code>payments_df</code> (pd.DataFrame): Aggregated payments data, typically the output of the ETL step. Expected columns include at least:</li> <li><code>sucursal</code> (branch name)</li> <li><code>fecha</code> (date or datetime)</li> <li>the metrics in config.metrics (e.g. ingreso_efectivo, ingreso_credito, ...)</li> <li><code>config</code> (Optional[ForecastConfig]): ForecastConfig for horizon, metrics, and branches. If None, uses defaults.</li> </ul>"},{"location":"api-reference/forecasting/#returns","title":"Returns","text":"<p>ForecastResult containing: - <code>forecast</code>: per-branch, per-metric predictions for the next horizon_days - <code>deposit_schedule</code>: computed cash-flow deposit schedule - <code>metadata</code>: additional information about the forecast</p>"},{"location":"api-reference/forecasting/#raises","title":"Raises","text":"<ul> <li><code>DataQualityError</code>: If required columns are missing, or no forecasts are generated.</li> </ul>"},{"location":"api-reference/forecasting/#example_1","title":"Example","text":"<pre><code>from pos_core.forecasting import ForecastConfig, run_payments_forecast\n\nconfig = ForecastConfig(horizon_days=7)\nresult = run_payments_forecast(payments_df, config=config)\n\nprint(result.forecast.head())\nprint(result.deposit_schedule)\n</code></pre>"},{"location":"api-reference/qa/","title":"QA API Reference","text":""},{"location":"api-reference/qa/#paymentsqaresult","title":"<code>PaymentsQAResult</code>","text":"<p>Result dataclass with QA summary and detailed findings.</p>"},{"location":"api-reference/qa/#attributes","title":"Attributes","text":"<ul> <li><code>summary</code> (dict): Dictionary with summary statistics and counts</li> <li><code>missing_days</code> (pd.DataFrame | None): DataFrame with missing days per sucursal, or None if none found</li> <li><code>duplicate_days</code> (pd.DataFrame | None): DataFrame with duplicate (sucursal, fecha) rows, or None if none found</li> <li><code>zscore_anomalies</code> (pd.DataFrame | None): DataFrame with z-score anomalies, or None if none found</li> <li><code>zero_method_flags</code> (pd.DataFrame | None): DataFrame with rows where tickets &gt; 0 but payment methods are zero, or None if none found</li> </ul>"},{"location":"api-reference/qa/#summary-keys","title":"Summary Keys","text":"<ul> <li><code>total_rows</code>: Total number of rows in the dataset</li> <li><code>total_sucursales</code>: Number of unique branches</li> <li><code>min_fecha</code>: Minimum date in the dataset</li> <li><code>max_fecha</code>: Maximum date in the dataset</li> <li><code>has_missing_days</code>: Boolean indicating if missing days were found</li> <li><code>has_duplicates</code>: Boolean indicating if duplicates were found</li> <li><code>has_zscore_anomalies</code>: Boolean indicating if z-score anomalies were found</li> <li><code>has_zero_method_flags</code>: Boolean indicating if zero method flags were found</li> <li><code>missing_days_count</code>: Number of missing days</li> <li><code>duplicate_days_count</code>: Number of duplicate days</li> <li><code>zscore_anomalies_count</code>: Number of z-score anomalies</li> <li><code>zero_method_flags_count</code>: Number of zero method flags</li> <li><code>schema_errors</code>: List of schema validation errors</li> </ul>"},{"location":"api-reference/qa/#run_payments_qa","title":"<code>run_payments_qa()</code>","text":"<p>Main QA function for data validation.</p>"},{"location":"api-reference/qa/#signature","title":"Signature","text":"<pre><code>def run_payments_qa(\n    payments_df: pd.DataFrame,\n    level: int = 4,\n) -&gt; PaymentsQAResult\n</code></pre>"},{"location":"api-reference/qa/#parameters","title":"Parameters","text":"<ul> <li><code>payments_df</code> (pd.DataFrame): Aggregated payments data, typically the output of the ETL step. Expected columns include at least:</li> <li><code>sucursal</code> (branch name)</li> <li><code>fecha</code> (date or datetime)</li> <li>payment method columns (ingreso_efectivo, ingreso_credito, etc.)</li> <li><code>num_tickets</code> (ticket count)</li> <li><code>level</code> (int): QA level to run (default: 4). Controls which checks are executed:</li> <li>Level 0: Schema validation (always run)</li> <li>Level 3: Missing and duplicate days</li> <li>Level 4: Statistical anomalies (z-score)</li> </ul>"},{"location":"api-reference/qa/#returns","title":"Returns","text":"<p>PaymentsQAResult containing: - <code>summary</code>: dictionary with counts and flags - <code>missing_days</code>: DataFrame with missing days per sucursal, or None - <code>duplicate_days</code>: DataFrame with duplicate rows, or None - <code>zscore_anomalies</code>: DataFrame with z-score anomalies, or None - <code>zero_method_flags</code>: DataFrame with zero method flags, or None</p>"},{"location":"api-reference/qa/#raises","title":"Raises","text":"<ul> <li><code>DataQualityError</code>: If required columns are missing.</li> </ul>"},{"location":"api-reference/qa/#example","title":"Example","text":"<pre><code>from pos_core.qa import run_payments_qa\n\nqa_result = run_payments_qa(payments_df, level=4)\n\nprint(f\"Missing days: {qa_result.summary['missing_days_count']}\")\nprint(f\"Anomalies: {qa_result.summary['zscore_anomalies_count']}\")\n\nif qa_result.missing_days is not None:\n    print(qa_result.missing_days)\n</code></pre>"},{"location":"user-guide/concepts/","title":"Concepts","text":"<p>This page explains key concepts and design decisions in POS Core ETL.</p>"},{"location":"user-guide/concepts/#branch-code-windows","title":"Branch Code Windows","text":"<p>Branches (sucursales) may change their codes over time. The package handles this through \"validity windows\" in <code>sucursales.json</code>.</p> <p>Each branch entry can specify: - <code>valid_from</code>: When this code became active - <code>valid_to</code>: When this code became inactive (null if still active)</p> <p>During extraction, the package automatically selects the correct code for each date range based on these windows.</p> <p>Example: If a branch changed codes on 2024-06-01, you'd have two entries:</p> <pre><code>{\n  \"MyBranch\": [\n    {\n      \"code\": \"1234\",\n      \"valid_from\": \"2024-01-01\",\n      \"valid_to\": \"2024-05-31\"\n    },\n    {\n      \"code\": \"5678\",\n      \"valid_from\": \"2024-06-01\",\n      \"valid_to\": null\n    }\n  ]\n}\n</code></pre>"},{"location":"user-guide/concepts/#etl-directory-convention","title":"ETL Directory Convention","text":"<p>The package uses a three-stage directory structure:</p> <ul> <li><code>a_raw/</code>: Raw data files downloaded from POS API (Excel files)</li> <li><code>b_clean/</code>: Cleaned and normalized data (CSV files)</li> <li><code>c_processed/</code>: Aggregated and processed datasets (CSV files)</li> </ul> <p>This convention makes it easy to: - Identify which stage each file belongs to - Re-run specific stages without re-processing everything - Debug issues at each stage</p>"},{"location":"user-guide/concepts/#api-layers","title":"API Layers","text":"<p>The package provides three levels of APIs:</p>"},{"location":"user-guide/concepts/#query-functions-recommended","title":"Query Functions (Recommended)","text":"<p>The query functions are the recommended way to get data:</p> <ul> <li><code>get_payments()</code>: Get payments data, running ETL stages only if needed</li> <li><code>get_sales()</code>: Get sales data at specified level, running ETL stages only if needed</li> <li><code>get_payments_forecast()</code>: Get payments forecast, automatically handling historical data</li> </ul> <p>Key features: - Automatic idempotence through metadata checks - Only runs ETL stages when needed - Simple, high-level interface - Returns DataFrames directly</p> <p>Example: <pre><code>from pos_core.etl import PaymentsETLConfig, get_payments\n\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\ndf = get_payments(\"2025-01-01\", \"2025-01-31\", config, refresh=False)\n</code></pre></p>"},{"location":"user-guide/concepts/#stage-functions-fine-grained-control","title":"Stage Functions (Fine-Grained Control)","text":"<p>Stage functions provide control over individual ETL stages:</p> <ul> <li>Payments: <code>download_payments()</code>, <code>clean_payments()</code>, <code>aggregate_payments()</code></li> <li>Sales: <code>download_sales()</code>, <code>clean_sales()</code>, <code>aggregate_sales()</code></li> </ul> <p>Use when: - You need to run specific stages independently - You want to inspect intermediate results - You're building custom workflows</p> <p>Example: <pre><code>from pos_core.etl import PaymentsETLConfig, download_payments, clean_payments\n\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\ndownload_payments(\"2025-01-01\", \"2025-01-31\", config)\nclean_payments(\"2025-01-01\", \"2025-01-31\", config)\n</code></pre></p>"},{"location":"user-guide/concepts/#low-level-functions-advanced","title":"Low-Level Functions (Advanced)","text":"<p>Low-level functions in <code>pos_core.etl.a_extract</code>, <code>pos_core.etl.b_transform</code>, and <code>pos_core.etl.c_load</code> provide direct access to extraction, transformation, and aggregation logic.</p> <p>Use when: - You need to customize the ETL logic - You're building custom pipelines - You need access to internal implementation details</p> <p>Note: These are considered internal APIs and may change between minor versions.</p>"},{"location":"user-guide/concepts/#metadata-and-idempotence","title":"Metadata and Idempotence","text":"<p>The ETL pipeline uses metadata files to track stage completion and enable idempotent operations.</p>"},{"location":"user-guide/concepts/#metadata-storage","title":"Metadata Storage","text":"<p>Metadata files are stored in <code>_meta/</code> subdirectories within each stage directory: - <code>a_raw/payments/_meta/2025-01-01_2025-01-31.json</code> - <code>b_clean/payments/_meta/2025-01-01_2025-01-31.json</code> - <code>c_processed/payments/_meta/2025-01-01_2025-01-31.json</code></p>"},{"location":"user-guide/concepts/#metadata-contents","title":"Metadata Contents","text":"<p>Each metadata file contains: - <code>start_date</code>, <code>end_date</code>: Date range processed - <code>branches</code>: List of branches processed - <code>cleaner_version</code>: Version identifier for the cleaner (enables re-cleaning when logic changes) - <code>last_run</code>: ISO timestamp of when the stage was run - <code>status</code>: \"ok\", \"failed\", or \"partial\"</p>"},{"location":"user-guide/concepts/#automatic-idempotence","title":"Automatic Idempotence","text":"<p>Query functions automatically check metadata: - If metadata exists and <code>status == \"ok\"</code> and <code>cleaner_version</code> matches, skip the stage - If <code>refresh=True</code>, force re-run all stages - If <code>refresh=False</code>, use existing data when available</p> <p>This makes it safe to re-run queries without duplicating work.</p>"},{"location":"user-guide/concepts/#pos-system-requirements","title":"POS System Requirements","text":"<p>This package is designed for POS systems that:</p> <ol> <li>Expose HTTP exports for:</li> <li>Payment reports</li> <li>Sales detail reports</li> <li> <p>Transfer reports</p> </li> <li> <p>Use Excel format for exported reports</p> </li> <li> <p>Support authentication via username/password (optional)</p> </li> </ol> <p>The package is currently optimized for Wansoft-style POS systems, but the architecture allows for future extension to other POS backends.</p>"},{"location":"user-guide/concepts/#incremental-processing","title":"Incremental Processing","text":"<p>The ETL pipeline is designed for incremental processing:</p> <ul> <li>Smart date range chunking: Automatically splits large date ranges into manageable chunks</li> <li>Existing data discovery: Skips downloading files that already exist</li> <li>Metadata-based idempotence: Tracks stage completion to avoid redundant work</li> <li>Resumable: Can be interrupted and resumed without losing progress</li> </ul> <p>This makes it practical to process years of historical data.</p>"},{"location":"user-guide/concepts/#forecasting-model","title":"Forecasting Model","text":"<p>The forecasting module uses ARIMA (AutoRegressive Integrated Moving Average) models:</p> <ul> <li>Log transformation: Applied to handle non-negative values</li> <li>Automatic hyperparameter selection: Searches for optimal ARIMA parameters</li> <li>Per-branch, per-metric: Separate models for each combination</li> </ul> <p>The models require at least 30 days of historical data to generate reliable forecasts.</p>"},{"location":"user-guide/configuration/","title":"Configuration","text":""},{"location":"user-guide/configuration/#branch-configuration-sucursalesjson","title":"Branch Configuration (sucursales.json)","text":"<p>The <code>sucursales.json</code> file maps branch names to codes and tracks validity windows.</p> <p>Default location: <code>utils/sucursales.json</code> (relative to your data root's parent directory)</p> <p>Example structure:</p> <pre><code>{\n  \"Banana\": {\n    \"code\": \"8888\",\n    \"valid_from\": \"2024-02-21\",\n    \"valid_to\": null\n  },\n  \"Queen\": {\n    \"code\": \"6362\",\n    \"valid_from\": \"2024-01-01\",\n    \"valid_to\": null\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#fields","title":"Fields","text":"<ul> <li><code>code</code>: Branch code used by the POS system</li> <li><code>valid_from</code>: Date when this code became active (YYYY-MM-DD format)</li> <li><code>valid_to</code>: Date when this code became inactive (null if still active)</li> </ul>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>Required for online extraction (downloading data from POS API):</p> <ul> <li><code>WS_BASE</code> (required): Base URL of your POS instance</li> <li><code>WS_USER</code> (optional): Username for authentication</li> <li><code>WS_PASS</code> (optional): Password for authentication</li> </ul> <p>Note: These are only needed if you're downloading data from the POS API. If you only work with already-downloaded files, you can ignore these.</p>"},{"location":"user-guide/configuration/#directory-structure","title":"Directory Structure","text":"<p>The package follows an ETL naming convention:</p> <pre><code>data/\n\u251c\u2500\u2500 a_raw/          # Raw data files downloaded from POS API\n\u2502   \u251c\u2500\u2500 payments/\n\u2502   \u2502   \u2514\u2500\u2500 batch/\n\u2502   \u2514\u2500\u2500 sales/\n\u2502       \u2514\u2500\u2500 batch/\n\u251c\u2500\u2500 b_clean/        # Cleaned and normalized data files\n\u2502   \u251c\u2500\u2500 payments/\n\u2502   \u2502   \u2514\u2500\u2500 batch/\n\u2502   \u2514\u2500\u2500 sales/\n\u2502       \u2514\u2500\u2500 batch/\n\u2514\u2500\u2500 c_processed/    # Aggregated and processed datasets\n    \u251c\u2500\u2500 payments/\n    \u2514\u2500\u2500 sales/\n</code></pre>"},{"location":"user-guide/configuration/#etl-configuration","title":"ETL Configuration","text":""},{"location":"user-guide/configuration/#payments-etl-configuration","title":"Payments ETL Configuration","text":"<pre><code>from pathlib import Path\nfrom pos_core.etl import PaymentsETLConfig\n\n# Default configuration using from_root (recommended)\nconfig = PaymentsETLConfig.from_root(\n    data_root=Path(\"data\"),\n    sucursales_file=Path(\"utils/sucursales.json\"),\n    chunk_size_days=180  # Maximum days per HTTP request\n)\n\n# Alternative: using from_data_root (alias)\nconfig = PaymentsETLConfig.from_data_root(\n    data_root=Path(\"data\"),\n    sucursales_json=Path(\"utils/sucursales.json\"),\n    chunk_size_days=180\n)\n\n# Custom configuration\nfrom pos_core.etl import PaymentsPaths\n\ncustom_paths = PaymentsPaths(\n    raw_payments=Path(\"custom/raw\"),\n    clean_payments=Path(\"custom/clean\"),\n    proc_payments=Path(\"custom/processed\"),\n    sucursales_json=Path(\"custom/sucursales.json\")\n)\n\nconfig = PaymentsETLConfig(\n    paths=custom_paths,\n    chunk_size_days=90,\n    excluded_branches=[\"CEDIS\"]  # Branches to exclude from processing\n)\n</code></pre>"},{"location":"user-guide/configuration/#sales-etl-configuration","title":"Sales ETL Configuration","text":"<pre><code>from pathlib import Path\nfrom pos_core.etl import SalesETLConfig\n\n# Default configuration\nconfig = SalesETLConfig.from_root(\n    data_root=Path(\"data\"),\n    sucursales_file=Path(\"utils/sucursales.json\")\n)\n\n# Custom configuration\nfrom pos_core.etl import SalesPaths\n\ncustom_paths = SalesPaths(\n    raw_sales=Path(\"custom/raw\"),\n    clean_sales=Path(\"custom/clean\"),\n    proc_sales=Path(\"custom/processed\"),\n    sucursales_json=Path(\"custom/sucursales.json\")\n)\n\nconfig = SalesETLConfig(\n    paths=custom_paths,\n    chunk_days=90  # Only if chunking is needed\n)\n</code></pre>"},{"location":"user-guide/configuration/#forecasting-configuration","title":"Forecasting Configuration","text":"<pre><code>from pos_core.forecasting import ForecastConfig\n\n# Default (7 days, all metrics, all branches)\nconfig = ForecastConfig()\n\n# Custom configuration\nconfig = ForecastConfig(\n    horizon_days=14,\n    metrics=[\"ingreso_efectivo\", \"ingreso_total\"],\n    branches=[\"Banana\", \"Queen\"]\n)\n</code></pre>"},{"location":"user-guide/examples/","title":"Examples","text":"<p>This package includes runnable example scripts in the <code>examples/</code> directory.</p>"},{"location":"user-guide/examples/#example-1-sales-detail-etl-query-api","title":"Example 1: Sales Detail ETL (Query API)","text":"<p>File: <code>examples/sales_week_by_group.py</code></p> <p>Demonstrates the high-level query API for sales detail ETL:</p> <ul> <li>Uses <code>get_sales()</code> to get sales data at different aggregation levels</li> <li>Automatically handles ETL stages (download, clean, aggregate) only when needed</li> <li>Shows both ticket-level and group-level aggregation</li> <li>Creates a pivot table (groups \u00d7 sucursales)</li> </ul> <p>Usage:</p> <pre><code>python examples/sales_week_by_group.py\n</code></pre> <p>Note: Modify <code>week_start</code> and <code>week_end</code> variables in the script.</p> <p>Key Features: - Uses <code>refresh=True</code> for the first call to ensure fresh data - Uses <code>refresh=False</code> for subsequent calls to reuse existing data - Demonstrates the <code>level</code> parameter (\"ticket\" vs \"group\")</p>"},{"location":"user-guide/examples/#example-2-payments-etl-query-api","title":"Example 2: Payments ETL (Query API)","text":"<p>File: <code>examples/payments_full_etl.py</code></p> <p>Demonstrates the high-level payments ETL API:</p> <ul> <li>Uses <code>get_payments()</code> to get payments data</li> <li>Automatically handles downloading, cleaning, and aggregating</li> <li>Uses metadata to skip work that's already been done</li> <li>Creates daily aggregated dataset</li> </ul> <p>Usage:</p> <pre><code>python examples/payments_full_etl.py\n</code></pre> <p>Note: Modify date range in the script if needed.</p> <p>Alternative: You can also use <code>build_payments_dataset()</code> for complete orchestration, or use stage functions (<code>download_payments</code>, <code>clean_payments</code>, <code>aggregate_payments</code>) for fine-grained control.</p>"},{"location":"user-guide/examples/#example-3-forecasting-query-api","title":"Example 3: Forecasting (Query API)","text":"<p>File: <code>examples/payments_forecast.py</code></p> <p>Demonstrates the forecasting query API:</p> <ul> <li>Uses <code>get_payments_forecast()</code> to get forecasts</li> <li>Automatically gets historical data and runs the forecast</li> <li>Returns forecast DataFrame directly</li> <li>Shows how to work with forecast results</li> </ul> <p>Usage:</p> <pre><code>python examples/payments_forecast.py\n</code></pre> <p>Note: The query API automatically handles getting historical data. For full control including deposit schedule and metadata, use <code>run_payments_forecast()</code> directly.</p>"},{"location":"user-guide/examples/#prerequisites","title":"Prerequisites","text":"<p>Before running any example:</p> <ol> <li>Install the package: <code>pip install -e .</code></li> <li>Create <code>utils/sucursales.json</code> (see Configuration)</li> <li>Set <code>WS_BASE</code> environment variable (for online extraction)</li> <li>Create data directory structure (or modify paths in scripts)</li> </ol> <p>See <code>examples/README.md</code> for more details.</p>"},{"location":"user-guide/examples/#advanced-stage-functions","title":"Advanced: Stage Functions","text":"<p>For fine-grained control, you can use stage functions directly:</p> <pre><code>from pos_core.etl import PaymentsETLConfig, download_payments, clean_payments, aggregate_payments\n\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Run stages individually\ndownload_payments(\"2025-01-01\", \"2025-01-31\", config)\nclean_payments(\"2025-01-01\", \"2025-01-31\", config)\ndf = aggregate_payments(\"2025-01-01\", \"2025-01-31\", config)\n</code></pre> <p>This gives you control over when each stage runs, but query functions (<code>get_payments</code>, <code>get_sales</code>) are recommended for most use cases as they handle idempotence automatically.</p>"},{"location":"user-guide/installation/","title":"Installation","text":""},{"location":"user-guide/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>POS system with Wansoft-style HTTP exports (see Concepts for details)</li> </ul>"},{"location":"user-guide/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install pos-core-etl\n</code></pre>"},{"location":"user-guide/installation/#install-for-development","title":"Install for Development","text":"<pre><code>git clone https://github.com/ToxicFyre/pos-pipeline-core-etl.git\ncd pos-pipeline-core-etl\npip install -e .[dev]\n</code></pre>"},{"location":"user-guide/installation/#dependencies","title":"Dependencies","text":"<p>The package requires: - pandas &gt;= 1.3.0 - numpy &gt;= 1.20.0 - requests &gt;= 2.25.0 - beautifulsoup4 &gt;= 4.9.0 - statsmodels &gt;= 0.12.0 - openpyxl &gt;= 3.0.0</p> <p>Optional dependencies (for development): - pytest &gt;= 7.0 - mypy &gt;= 1.0.0 - ruff &gt;= 0.1.0 - black &gt;= 23.0.0</p>"},{"location":"user-guide/installation/#next-steps","title":"Next Steps","text":"<ol> <li>Create your <code>sucursales.json</code> configuration file (see Configuration)</li> <li>Set up your data directory structure (see Concepts)</li> <li>Try the Quickstart guide</li> </ol>"},{"location":"user-guide/quickstart/","title":"Quickstart","text":"<p>Get started with POS Core ETL in minutes.</p>"},{"location":"user-guide/quickstart/#basic-etl-workflow","title":"Basic ETL Workflow","text":"<p>The recommended approach is to use the query functions, which automatically handle running ETL stages and provide automatic idempotence:</p> <pre><code>from pathlib import Path\nfrom pos_core.etl import PaymentsETLConfig, get_payments\n\n# Configure\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Get payments data (automatically runs ETL stages only if needed)\npayments = get_payments(\n    start_date=\"2025-01-01\",\n    end_date=\"2025-01-31\",\n    config=config,\n    refresh=False,  # Use existing data if available\n)\n\nprint(f\"Processed {len(payments)} rows\")\n</code></pre> <p>Alternative: Use <code>build_payments_dataset()</code> for complete orchestration:</p> <pre><code>from pos_core.etl import PaymentsETLConfig, build_payments_dataset\n\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\npayments = build_payments_dataset(\"2025-01-01\", \"2025-01-31\", config)\n</code></pre>"},{"location":"user-guide/quickstart/#sales-data","title":"Sales Data","text":"<p>Get sales data aggregated at different levels:</p> <pre><code>from pos_core.etl import SalesETLConfig, get_sales\n\nconfig = SalesETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Get sales by ticket\ndf_ticket = get_sales(\"2025-01-01\", \"2025-01-31\", config, level=\"ticket\")\n\n# Get sales by product group (pivot table)\ndf_group = get_sales(\"2025-01-01\", \"2025-01-31\", config, level=\"group\")\n</code></pre>"},{"location":"user-guide/quickstart/#forecasting","title":"Forecasting","text":"<p>The easiest way to get forecasts is using the query API:</p> <pre><code>from pos_core.etl import PaymentsETLConfig, get_payments_forecast\n\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Get 13-week forecast (automatically gets historical data)\nforecast = get_payments_forecast(\n    as_of=\"2025-11-24\",\n    horizon_weeks=13,\n    config=config,\n)\n\nprint(forecast.head())\n</code></pre> <p>Alternative: For full control including deposit schedule and metadata:</p> <pre><code>from pos_core.etl import PaymentsETLConfig, get_payments\nfrom pos_core.forecasting import ForecastConfig, run_payments_forecast\n\n# Get historical data\npayments = get_payments(\"2022-01-01\", \"2025-11-24\", config)\n\n# Run forecast with full result\nresult = run_payments_forecast(\n    payments,\n    ForecastConfig(horizon_days=91)  # 13 weeks\n)\n\nprint(result.forecast.head())\nprint(result.deposit_schedule)\n</code></pre>"},{"location":"user-guide/quickstart/#quality-assurance","title":"Quality Assurance","text":"<pre><code>from pos_core.etl import PaymentsETLConfig, get_payments\nfrom pos_core.qa import run_payments_qa\n\n# Get payments data\nconfig = PaymentsETLConfig.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\npayments = get_payments(\"2025-01-01\", \"2025-01-31\", config)\n\n# Run QA checks\nqa_result = run_payments_qa(payments)\n\nprint(f\"Missing days: {qa_result.summary['missing_days_count']}\")\nprint(f\"Anomalies: {qa_result.summary['zscore_anomalies_count']}\")\n</code></pre>"},{"location":"user-guide/quickstart/#see-also","title":"See Also","text":"<ul> <li>Configuration - Detailed configuration options</li> <li>Examples - Complete runnable examples</li> <li>API Reference - Full API documentation</li> </ul>"}]}