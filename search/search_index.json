{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"POS Core ETL Documentation","text":"<p>Welcome to the POS Core ETL documentation! This package provides a comprehensive solution for Point of Sale data processing, forecasting, and quality assurance.</p>"},{"location":"#what-is-pos-core-etl","title":"What is POS Core ETL?","text":"<p>POS Core ETL is a Python package designed for POS systems that expose Wansoft-style HTTP exports. It provides:</p> <ul> <li>ETL Pipeline: Extract, transform, and load POS payment and sales data across bronze/silver/gold layers</li> <li>Time Series Forecasting: Generate ARIMA-based forecasts for payment metrics</li> <li>Quality Assurance: Automated data validation and anomaly detection</li> <li>Multi-Branch Support: Handle multiple sucursales (branches) with code window tracking</li> <li>Incremental Processing: Smart date range chunking and existing data discovery</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import marts as payments_marts\nfrom pos_core.forecasting import ForecastConfig, run_payments_forecast\n\n# Configure paths\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Get payments daily mart\npayments_daily = payments_marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Generate forecast\nconfig = ForecastConfig(horizon_days=91)\nresult = run_payments_forecast(payments_daily, config)\nprint(result.forecast.head())\n</code></pre>"},{"location":"#documentation-structure","title":"Documentation Structure","text":""},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":"<ul> <li>Installation - Install the package and set up your environment</li> <li>Quickstart - Get started in minutes with a working example</li> <li>Configuration - Configure branches, credentials, and paths</li> <li>Examples - Complete runnable example scripts</li> </ul>"},{"location":"#concepts","title":"\ud83d\udcda Concepts","text":"<ul> <li>Concepts - Understand data layers, grains, API design, and key concepts</li> </ul>"},{"location":"#api-reference","title":"\ud83d\udd27 API Reference","text":"<ul> <li>ETL API - ETL pipeline functions and configuration</li> <li>Forecasting API - Time series forecasting functions</li> <li>QA API - Quality assurance and validation functions</li> <li>Exceptions - Error handling and exceptions</li> </ul>"},{"location":"#development","title":"\ud83d\udee0\ufe0f Development","text":"<ul> <li>Development Notes - Information for contributors</li> </ul>"},{"location":"#key-concepts","title":"Key Concepts","text":""},{"location":"#data-layers","title":"Data Layers","text":"<p>The package follows industry-standard bronze/silver/gold data layer conventions:</p> <ul> <li>Bronze: Raw Wansoft exports (Excel files in <code>a_raw/</code>)</li> <li>Silver (Core): Core facts at atomic grain (CSV files in <code>b_clean/</code>)</li> <li>Gold (Marts): Aggregated tables for analysis (CSV files in <code>c_processed/</code>)</li> </ul>"},{"location":"#api-design","title":"API Design","text":"<p>The package uses domain + layer modules:</p> <ul> <li><code>pos_core.payments.core</code> \u2192 payments, silver (core fact)</li> <li><code>pos_core.payments.marts</code> \u2192 payments, gold (aggregates)</li> <li><code>pos_core.sales.core</code> \u2192 sales, silver</li> <li><code>pos_core.sales.marts</code> \u2192 sales, gold</li> </ul> <p>Functions: - <code>fetch()</code> / <code>fetch_*</code>: Run ETL for missing partitions - <code>load()</code> / <code>load_*</code>: Read existing outputs only</p>"},{"location":"#data-grains","title":"Data Grains","text":"Domain Core Fact Grain Payments <code>fact_payments_ticket</code> ticket \u00d7 payment method Sales <code>fact_sales_item_line</code> item/modifier line"},{"location":"#common-use-cases","title":"Common Use Cases","text":""},{"location":"#payments-etl","title":"Payments ETL","text":"<pre><code>from pos_core.payments import core, marts\n\n# Daily mart (most common)\ndaily_df = marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Core fact\nfact_df = core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"#sales-etl","title":"Sales ETL","text":"<pre><code>from pos_core.sales import core, marts\n\n# Core fact\nfact_df = core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Ticket mart\nticket_df = marts.fetch_ticket(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Group mart\ngroup_df = marts.fetch_group(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"#forecasting","title":"Forecasting","text":"<pre><code>from pos_core.forecasting import ForecastConfig, run_payments_forecast\n\nconfig = ForecastConfig(horizon_days=91)\nresult = run_payments_forecast(payments_df, config)\n</code></pre>"},{"location":"#quality-assurance","title":"Quality Assurance","text":"<pre><code>from pos_core.qa import run_payments_qa\n\nresult = run_payments_qa(payments_df)\nprint(result.summary)\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>pandas &gt;= 1.3.0</li> <li>numpy &gt;= 1.20.0</li> <li>requests &gt;= 2.25.0</li> <li>beautifulsoup4 &gt;= 4.9.0</li> <li>statsmodels &gt;= 0.12.0</li> <li>openpyxl &gt;= 3.0.0</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pos-core-etl\n</code></pre> <p>For development:</p> <pre><code>git clone https://github.com/ToxicFyre/pos-pipeline-core-etl.git\ncd pos-pipeline-core-etl\npip install -e .[dev]\n</code></pre>"},{"location":"#support","title":"Support","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Source Code: <code>src/pos_core/</code></li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - see LICENSE file for details.</p>"},{"location":"api-reference/etl/","title":"ETL API Reference","text":"<p>This page documents the ETL API for POS Core ETL. The API is organized by domain (payments, sales) and layer (raw, core, marts).</p>"},{"location":"api-reference/etl/#configuration","title":"Configuration","text":""},{"location":"api-reference/etl/#datapaths","title":"<code>DataPaths</code>","text":"<p>Configuration class for data directory paths.</p>"},{"location":"api-reference/etl/#methods","title":"Methods","text":""},{"location":"api-reference/etl/#from_root","title":"<code>from_root()</code>","text":"<p>Create a DataPaths instance from root directories.</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\n\npaths = DataPaths.from_root(\n    data_root=Path(\"data\"),\n    sucursales_file=Path(\"utils/sucursales.json\")\n)\n</code></pre> <p>Parameters: - <code>data_root</code> (Path): Root directory for data (contains <code>a_raw/</code>, <code>b_clean/</code>, <code>c_processed/</code>) - <code>sucursales_file</code> (Path): Path to <code>sucursales.json</code> configuration file</p> <p>Returns: DataPaths instance</p>"},{"location":"api-reference/etl/#attributes","title":"Attributes","text":"<ul> <li><code>raw_payments</code> (Path): Directory for raw payment Excel files (<code>a_raw/payments/</code>)</li> <li><code>raw_sales</code> (Path): Directory for raw sales Excel files (<code>a_raw/sales/</code>)</li> <li><code>raw_transfers</code> (Path): Directory for raw transfer Excel files (<code>a_raw/transfers/</code>)</li> <li><code>raw_order_times</code> (Path): Directory for raw order times Excel files (<code>a_raw/order_times/</code>)</li> <li><code>clean_payments</code> (Path): Directory for cleaned payment CSV files (<code>b_clean/payments/</code>)</li> <li><code>clean_sales</code> (Path): Directory for cleaned sales CSV files (<code>b_clean/sales/</code>)</li> <li><code>clean_transfers</code> (Path): Directory for cleaned transfer CSV files (<code>b_clean/transfers/</code>)</li> <li><code>clean_order_times</code> (Path): Directory for cleaned order times CSV files (<code>b_clean/order_times/</code>)</li> <li><code>mart_payments</code> (Path): Directory for payment marts (<code>c_processed/payments/</code>)</li> <li><code>mart_sales</code> (Path): Directory for sales marts (<code>c_processed/sales/</code>)</li> <li><code>mart_transfers</code> (Path): Directory for transfer marts (<code>c_processed/transfers/</code>)</li> <li><code>mart_order_times</code> (Path): Directory for order times marts (<code>c_processed/order_times/</code>)</li> <li><code>sucursales_json</code> (Path): Path to <code>sucursales.json</code> file</li> </ul>"},{"location":"api-reference/etl/#payments-api","title":"Payments API","text":""},{"location":"api-reference/etl/#core-fact-silver-layer","title":"Core Fact (Silver Layer)","text":""},{"location":"api-reference/etl/#paymentscorefetch","title":"<code>payments.core.fetch()</code>","text":"<p>Ensure <code>fact_payments_ticket</code> exists for the given range, then return it.</p> <p>Signature:</p> <pre><code>from pos_core.payments import core\n\ndf = core.fetch(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: - <code>paths</code> (DataPaths): DataPaths configuration - <code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive) - <code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive) - <code>branches</code> (list[str] | None): Optional list of branch names to filter - <code>mode</code> (str): Processing mode - <code>\"missing\"</code> (default) or <code>\"force\"</code></p> <p>Returns: DataFrame with <code>fact_payments_ticket</code> structure (ticket \u00d7 payment method grain)</p> <p>Raises: - <code>ValueError</code>: If mode is not <code>\"missing\"</code> or <code>\"force\"</code></p> <p>Example:</p> <pre><code>from pos_core import DataPaths\nfrom pos_core.payments import core\nfrom pathlib import Path\n\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\ndf = core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"api-reference/etl/#paymentscoreload","title":"<code>payments.core.load()</code>","text":"<p>Load <code>fact_payments_ticket</code> from disk without running ETL.</p> <p>Signature:</p> <pre><code>df = core.load(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: Same as <code>fetch()</code>, except no <code>mode</code> parameter</p> <p>Returns: DataFrame with <code>fact_payments_ticket</code> structure</p> <p>Raises: - <code>FileNotFoundError</code>: If the data doesn't exist</p> <p>Example:</p> <pre><code># Read existing data only (faster, but requires data to exist)\ndf = core.load(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"api-reference/etl/#daily-mart-gold-layer","title":"Daily Mart (Gold Layer)","text":""},{"location":"api-reference/etl/#paymentsmartsfetch_daily","title":"<code>payments.marts.fetch_daily()</code>","text":"<p>Ensure the daily payments mart exists for the range, then return it.</p> <p>Signature:</p> <pre><code>from pos_core.payments import marts\n\ndf = marts.fetch_daily(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: - <code>paths</code> (DataPaths): DataPaths configuration - <code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive) - <code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive) - <code>branches</code> (list[str] | None): Optional list of branch names to filter - <code>mode</code> (str): Processing mode - <code>\"missing\"</code> (default) or <code>\"force\"</code></p> <p>Returns: DataFrame with <code>mart_payments_daily</code> structure (sucursal \u00d7 date grain)</p> <p>Example:</p> <pre><code>from pos_core.payments import marts\n\n# Most common use case: get daily aggregations\ndf = marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Force refresh\ndf = marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\", mode=\"force\")\n\n# Filter by branches\ndf = marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\", branches=[\"Banana\", \"Queen\"])\n</code></pre>"},{"location":"api-reference/etl/#paymentsmartsload_daily","title":"<code>payments.marts.load_daily()</code>","text":"<p>Load daily payments mart from disk without running ETL.</p> <p>Signature:</p> <pre><code>df = marts.load_daily(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: Same as <code>fetch_daily()</code>, except no <code>mode</code> parameter</p> <p>Returns: DataFrame with <code>mart_payments_daily</code> structure</p> <p>Raises: - <code>FileNotFoundError</code>: If the mart doesn't exist</p>"},{"location":"api-reference/etl/#raw-data-bronze-layer","title":"Raw Data (Bronze Layer)","text":""},{"location":"api-reference/etl/#paymentsrawfetch","title":"<code>payments.raw.fetch()</code>","text":"<p>Download raw payment Excel files from the POS system.</p> <p>Signature:</p> <pre><code>from pos_core.payments import raw\n\nraw.fetch(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; None\n</code></pre> <p>Parameters: - <code>paths</code> (DataPaths): DataPaths configuration - <code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive) - <code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive) - <code>branches</code> (list[str] | None): Optional list of branch names to filter - <code>mode</code> (str): Processing mode - <code>\"missing\"</code> (default) or <code>\"force\"</code></p> <p>Note: Requires <code>WS_BASE</code>, <code>WS_USER</code>, and <code>WS_PASS</code> environment variables to be set.</p>"},{"location":"api-reference/etl/#paymentsrawload","title":"<code>payments.raw.load()</code>","text":"<p>Load raw payment Excel files from disk.</p> <p>Signature:</p> <pre><code>df = raw.load(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#sales-api","title":"Sales API","text":""},{"location":"api-reference/etl/#core-fact-silver-layer_1","title":"Core Fact (Silver Layer)","text":""},{"location":"api-reference/etl/#salescorefetch","title":"<code>sales.core.fetch()</code>","text":"<p>Ensure <code>fact_sales_item_line</code> exists for the given range, then return it.</p> <p>Signature:</p> <pre><code>from pos_core.sales import core\n\ndf = core.fetch(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: - <code>paths</code> (DataPaths): DataPaths configuration - <code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive) - <code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive) - <code>branches</code> (list[str] | None): Optional list of branch names to filter - <code>mode</code> (str): Processing mode - <code>\"missing\"</code> (default) or <code>\"force\"</code></p> <p>Returns: DataFrame with <code>fact_sales_item_line</code> structure (item/modifier line grain)</p> <p>Example:</p> <pre><code>from pos_core.sales import core\n\ndf = core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"api-reference/etl/#salescoreload","title":"<code>sales.core.load()</code>","text":"<p>Load <code>fact_sales_item_line</code> from disk without running ETL.</p> <p>Signature:</p> <pre><code>df = core.load(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#ticket-mart-gold-layer","title":"Ticket Mart (Gold Layer)","text":""},{"location":"api-reference/etl/#salesmartsfetch_ticket","title":"<code>sales.marts.fetch_ticket()</code>","text":"<p>Ensure the ticket-level sales mart exists for the range, then return it.</p> <p>Signature:</p> <pre><code>from pos_core.sales import marts\n\ndf = marts.fetch_ticket(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: Same as <code>sales.core.fetch()</code></p> <p>Returns: DataFrame with <code>mart_sales_by_ticket</code> structure (one row per ticket)</p> <p>Example:</p> <pre><code>df = marts.fetch_ticket(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"api-reference/etl/#salesmartsload_ticket","title":"<code>sales.marts.load_ticket()</code>","text":"<p>Load ticket-level sales mart from disk without running ETL.</p> <p>Signature:</p> <pre><code>df = marts.load_ticket(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#group-mart-gold-layer","title":"Group Mart (Gold Layer)","text":""},{"location":"api-reference/etl/#salesmartsfetch_group","title":"<code>sales.marts.fetch_group()</code>","text":"<p>Ensure the group-level sales mart exists for the range, then return it.</p> <p>Signature:</p> <pre><code>df = marts.fetch_group(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: Same as <code>sales.core.fetch()</code></p> <p>Returns: DataFrame with <code>mart_sales_by_group</code> structure (category pivot table)</p> <p>Example:</p> <pre><code>df = marts.fetch_group(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"api-reference/etl/#salesmartsload_group","title":"<code>sales.marts.load_group()</code>","text":"<p>Load group-level sales mart from disk without running ETL.</p> <p>Signature:</p> <pre><code>df = marts.load_group(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#raw-data-bronze-layer_1","title":"Raw Data (Bronze Layer)","text":""},{"location":"api-reference/etl/#salesrawfetch","title":"<code>sales.raw.fetch()</code>","text":"<p>Download raw sales Excel files from the POS system.</p> <p>Signature:</p> <pre><code>from pos_core.sales import raw\n\nraw.fetch(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; None\n</code></pre> <p>Note: Requires <code>WS_BASE</code>, <code>WS_USER</code>, and <code>WS_PASS</code> environment variables to be set.</p>"},{"location":"api-reference/etl/#salesrawload","title":"<code>sales.raw.load()</code>","text":"<p>Load raw sales Excel files from disk.</p> <p>Signature:</p> <pre><code>df = raw.load(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n) -&gt; pd.DataFrame\n</code></pre>"},{"location":"api-reference/etl/#transfers-api","title":"Transfers API","text":""},{"location":"api-reference/etl/#core-fact-silver-layer_2","title":"Core Fact (Silver Layer)","text":""},{"location":"api-reference/etl/#transferscorefetch","title":"<code>transfers.core.fetch()</code>","text":"<p>Ensure <code>fact_transfers_line</code> exists for the given range, then return it.</p> <p>Signature:</p> <pre><code>from pos_core.transfers import core\n\ndf = core.fetch(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: - <code>paths</code> (DataPaths): DataPaths configuration - <code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive) - <code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive) - <code>branches</code> (list[str] | None): Optional list of branch names to filter - <code>mode</code> (str): Processing mode - <code>\"missing\"</code> (default) or <code>\"force\"</code></p> <p>Returns: DataFrame with <code>fact_transfers_line</code> structure (transfer line grain)</p> <p>Example:</p> <pre><code>from pos_core.transfers import core\n\ndf = core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"api-reference/etl/#transferscoreload","title":"<code>transfers.core.load()</code>","text":"<p>Load <code>fact_transfers_line</code> from disk without running ETL.</p> <p>Signature:</p> <pre><code>df = core.load(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: Same as <code>fetch()</code>, except no <code>mode</code> parameter</p> <p>Returns: DataFrame with <code>fact_transfers_line</code> structure</p> <p>Raises: - <code>FileNotFoundError</code>: If the data doesn't exist</p>"},{"location":"api-reference/etl/#pivot-mart-gold-layer","title":"Pivot Mart (Gold Layer)","text":""},{"location":"api-reference/etl/#transfersmartsfetch_pivot","title":"<code>transfers.marts.fetch_pivot()</code>","text":"<p>Ensure the transfer pivot mart exists for the range, then return it.</p> <p>Signature:</p> <pre><code>from pos_core.transfers import marts\n\ndf = marts.fetch_pivot(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n    include_cedis: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: - <code>paths</code> (DataPaths): DataPaths configuration - <code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive) - <code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive) - <code>branches</code> (list[str] | None): Optional list of branch names to filter - <code>mode</code> (str): Processing mode - <code>\"missing\"</code> (default) or <code>\"force\"</code> - <code>include_cedis</code> (bool): If True, include rows where destination is CEDIS (default: False)</p> <p>Returns: DataFrame with <code>mart_transfers_pivot</code> structure (branch \u00d7 category pivot)</p> <p>Example:</p> <pre><code>from pos_core.transfers import marts\n\n# Get pivot mart\ndf = marts.fetch_pivot(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Force refresh\ndf = marts.fetch_pivot(paths, \"2025-01-01\", \"2025-01-31\", mode=\"force\")\n\n# Include CEDIS as destination\ndf = marts.fetch_pivot(paths, \"2025-01-01\", \"2025-01-31\", include_cedis=True)\n</code></pre>"},{"location":"api-reference/etl/#transfersmartsload_pivot","title":"<code>transfers.marts.load_pivot()</code>","text":"<p>Load the transfer pivot mart from disk without running ETL.</p> <p>Signature:</p> <pre><code>df = marts.load_pivot(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Parameters: Same as <code>fetch_pivot()</code>, except no <code>mode</code> or <code>include_cedis</code> parameter</p> <p>Returns: DataFrame with <code>mart_transfers_pivot</code> structure</p> <p>Raises: - <code>FileNotFoundError</code>: If the mart doesn't exist</p>"},{"location":"api-reference/etl/#raw-data-bronze-layer_2","title":"Raw Data (Bronze Layer)","text":""},{"location":"api-reference/etl/#transfersrawfetch","title":"<code>transfers.raw.fetch()</code>","text":"<p>Download raw transfer Excel files from the POS system.</p> <p>Signature:</p> <pre><code>from pos_core.transfers import raw\n\nraw.fetch(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; None\n</code></pre> <p>Parameters: - <code>paths</code> (DataPaths): DataPaths configuration - <code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive) - <code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive) - <code>branches</code> (list[str] | None): Optional list of branch names to filter - <code>mode</code> (str): Processing mode - <code>\"missing\"</code> (default) or <code>\"force\"</code></p> <p>Note: Requires <code>WS_BASE</code>, <code>WS_USER</code>, and <code>WS_PASS</code> environment variables to be set.</p>"},{"location":"api-reference/etl/#transfersrawload","title":"<code>transfers.raw.load()</code>","text":"<p>Verify that raw transfer data exists for the given range.</p> <p>Signature:</p> <pre><code>raw.load(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n) -&gt; None\n</code></pre> <p>Raises: - <code>FileNotFoundError</code>: If required raw transfer files are missing</p>"},{"location":"api-reference/etl/#order-times-api","title":"Order Times API","text":""},{"location":"api-reference/etl/#raw-data-bronze-layer_3","title":"Raw Data (Bronze Layer)","text":""},{"location":"api-reference/etl/#order_timesrawfetch","title":"<code>order_times.raw.fetch()</code>","text":"<p>Download raw order times Excel files from the POS system.</p> <p>Signature:</p> <pre><code>from pos_core.order_times import raw\n\nraw.fetch(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n    branches: list[str] | None = None,\n    *,\n    mode: str = \"missing\",\n) -&gt; None\n</code></pre> <p>Parameters: - <code>paths</code> (DataPaths): DataPaths configuration - <code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive) - <code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive) - <code>branches</code> (list[str] | None): Optional list of branch names to filter - <code>mode</code> (str): Processing mode - <code>\"missing\"</code> (default) or <code>\"force\"</code></p> <p>Note: Requires <code>WS_BASE</code>, <code>WS_USER</code>, and <code>WS_PASS</code> environment variables to be set.</p> <p>Example:</p> <pre><code>from pos_core.order_times import raw\n\n# Download order times for a date range\nraw.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Download for specific branches\nraw.fetch(paths, \"2025-01-01\", \"2025-01-31\", branches=[\"Punto Valle\"])\n\n# Force re-download\nraw.fetch(paths, \"2025-01-01\", \"2025-01-31\", mode=\"force\")\n</code></pre>"},{"location":"api-reference/etl/#order_timesrawload","title":"<code>order_times.raw.load()</code>","text":"<p>Verify that raw order times data exists for the given range.</p> <p>Signature:</p> <pre><code>raw.load(\n    paths: DataPaths,\n    start_date: str,\n    end_date: str,\n) -&gt; None\n</code></pre> <p>Parameters: - <code>paths</code> (DataPaths): DataPaths configuration - <code>start_date</code> (str): Start date in YYYY-MM-DD format (inclusive) - <code>end_date</code> (str): End date in YYYY-MM-DD format (inclusive)</p> <p>Raises: - <code>FileNotFoundError</code>: If required raw order times files are missing</p> <p>Example:</p> <pre><code># Verify data exists (raises error if missing)\nraw.load(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"api-reference/etl/#processing-modes","title":"Processing Modes","text":""},{"location":"api-reference/etl/#modemissing-default","title":"<code>mode=\"missing\"</code> (Default)","text":"<ul> <li>Only runs ETL stages for date ranges that don't have completed outputs</li> <li>Checks metadata to determine if stages need to run</li> <li>Skips work that's already been done</li> <li>Recommended for most use cases</li> </ul>"},{"location":"api-reference/etl/#modeforce","title":"<code>mode=\"force\"</code>","text":"<ul> <li>Forces re-run of all ETL stages for the given date range</li> <li>Ignores existing metadata and outputs</li> <li>Useful when:</li> <li>You've fixed a bug in transformation logic</li> <li>You want to refresh data from source</li> <li>You're debugging ETL issues</li> </ul>"},{"location":"api-reference/etl/#function-behavior","title":"Function Behavior","text":""},{"location":"api-reference/etl/#fetch-functions","title":"<code>fetch()</code> Functions","text":"<ul> <li>May run ETL: Checks if data exists, runs ETL if needed (based on mode)</li> <li>Idempotent: Safe to call multiple times</li> <li>Returns DataFrame: Always returns the requested data</li> </ul>"},{"location":"api-reference/etl/#load-functions","title":"<code>load()</code> Functions","text":"<ul> <li>Never runs ETL: Only reads existing data from disk</li> <li>Faster: No ETL overhead</li> <li>Raises error: If data doesn't exist</li> <li>Use when: You're certain the data already exists</li> </ul>"},{"location":"api-reference/etl/#data-structures","title":"Data Structures","text":""},{"location":"api-reference/etl/#fact_payments_ticket-core-fact","title":"<code>fact_payments_ticket</code> (Core Fact)","text":"<p>Grain: ticket \u00d7 payment method</p> <p>Key columns: - <code>sucursal</code>: Branch name - <code>operating_date</code>: Date of operation - <code>order_index</code>: Ticket/order identifier - <code>payment_method</code>: Payment method (e.g., \"Efectivo\", \"Tarjeta Cr\u00e9dito\")</p>"},{"location":"api-reference/etl/#mart_payments_daily-daily-mart","title":"<code>mart_payments_daily</code> (Daily Mart)","text":"<p>Grain: sucursal \u00d7 date</p> <p>Key columns: - <code>sucursal</code>: Branch name - <code>fecha</code>: Date - <code>ingreso_efectivo</code>: Cash income - <code>ingreso_credito</code>: Credit card income - <code>ingreso_debito</code>: Debit card income - <code>num_tickets</code>: Number of tickets - Additional payment method columns</p>"},{"location":"api-reference/etl/#fact_sales_item_line-core-fact","title":"<code>fact_sales_item_line</code> (Core Fact)","text":"<p>Grain: item/modifier line</p> <p>Key columns: - <code>sucursal</code>: Branch name - <code>operating_date</code>: Date of operation - <code>order_id</code>: Ticket/order identifier - <code>item_key</code>: Item identifier - <code>group</code>: Product group/category - <code>subtotal_item</code>: Item subtotal - <code>total_item</code>: Item total</p>"},{"location":"api-reference/etl/#mart_sales_by_ticket-ticket-mart","title":"<code>mart_sales_by_ticket</code> (Ticket Mart)","text":"<p>Grain: one row per ticket</p> <p>Key columns: - <code>sucursal</code>: Branch name - <code>operating_date</code>: Date of operation - <code>order_id</code>: Ticket/order identifier - <code>total</code>: Ticket total - Additional aggregated columns</p>"},{"location":"api-reference/etl/#mart_sales_by_group-group-mart","title":"<code>mart_sales_by_group</code> (Group Mart)","text":"<p>Grain: category pivot table</p> <p>Structure: Groups as columns, sucursales/dates as rows</p>"},{"location":"api-reference/etl/#fact_transfers_line-core-fact","title":"<code>fact_transfers_line</code> (Core Fact)","text":"<p>Grain: transfer line item</p> <p>Key columns: - <code>Orden</code>: Transfer order number - <code>Almac\u00e9n origen</code>: Origin warehouse - <code>Sucursal destino</code>: Destination branch - <code>Almac\u00e9n destino</code>: Destination warehouse - <code>Fecha</code>: Transfer date - <code>Estatus</code>: Transfer status - <code>Cantidad</code>: Quantity transferred - <code>Departamento</code>: Product department - <code>Clave</code>: Product code - <code>Producto</code>: Product name - <code>Presentaci\u00f3n</code>: Product presentation - <code>Costo</code>: Total cost - <code>Costo unitario</code>: Unit cost</p>"},{"location":"api-reference/etl/#mart_transfers_pivot-pivot-mart","title":"<code>mart_transfers_pivot</code> (Pivot Mart)","text":"<p>Grain: branch \u00d7 category pivot table</p> <p>Structure: - Rows: Product categories (NO-PROC, REFRICONGE, TOSTADOR, COMIDA SALADA, REPO, PAN DULCE, PAN SALADA, PAN DULCE Y SALADA, TOTAL) - Columns: Branch codes (Kavia, PV, Qin, Zambrano, Carreta, Nativa, Crediclub, TOTAL)</p> <p>Category mapping: - <code>ALMACEN PRODUCTO TERMINADO</code> + <code>COCINA</code> \u2192 COMIDA SALADA - <code>ALMACEN PRODUCTO TERMINADO</code> + <code>REPOSTERIA</code> \u2192 REPO - <code>ALMACEN PRODUCTO TERMINADO</code> + <code>PAN DULCE</code> \u2192 PAN DULCE (new format) - <code>ALMACEN PRODUCTO TERMINADO</code> + <code>PAN SALADA</code> \u2192 PAN SALADA (new format) - <code>ALMACEN PRODUCTO TERMINADO</code> + <code>PANADERIA DULCE Y SALADA</code> \u2192 PAN DULCE Y SALADA (legacy format, backward compatibility) - <code>ALMACEN GENERAL</code> + various departments \u2192 NO-PROC, REFRICONGE, TOSTADOR</p>"},{"location":"api-reference/etl/#next-steps","title":"Next Steps","text":"<ul> <li>Forecasting API - Generate time series forecasts</li> <li>QA API - Data quality assurance</li> <li>Concepts - Understand data layers and grains</li> </ul>"},{"location":"api-reference/exceptions/","title":"Exceptions API Reference","text":"<p>The package defines custom exceptions that are part of the public API. All exceptions inherit from <code>PosAPIError</code> for easy catching.</p>"},{"location":"api-reference/exceptions/#posapierror","title":"<code>PosAPIError</code>","text":"<p>Base exception for all POS Core ETL errors.</p> <p>This is the base class for all domain-specific exceptions in the package. Users can catch this exception to handle any POS Core ETL error.</p> <pre><code>from pos_core.exceptions import PosAPIError\n\ntry:\n    # Some POS Core ETL operation\n    pass\nexcept PosAPIError as e:\n    # Handle any POS Core ETL error\n    print(f\"POS Core ETL error: {e}\")\n</code></pre>"},{"location":"api-reference/exceptions/#configerror","title":"<code>ConfigError</code>","text":"<p>Raised when there is a configuration error.</p> <p>This exception is raised when: - Invalid configuration values are provided - Required configuration is missing - Configuration files cannot be loaded or parsed</p> <pre><code>from pos_core.exceptions import ConfigError\n\ntry:\n    # Configuration operation\n    pass\nexcept ConfigError as e:\n    # Handle configuration error\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"api-reference/exceptions/#dataqualityerror","title":"<code>DataQualityError</code>","text":"<p>Raised when data quality checks fail.</p> <p>This exception is raised when: - Required columns are missing from input data - Data validation fails - Data quality checks detect critical issues</p> <pre><code>from pos_core.exceptions import DataQualityError\n\ntry:\n    # Data quality operation\n    pass\nexcept DataQualityError as e:\n    # Handle data quality error\n    print(f\"Data quality error: {e}\")\n</code></pre>"},{"location":"api-reference/forecasting/","title":"Forecasting API Reference","text":""},{"location":"api-reference/forecasting/#forecastconfig","title":"<code>ForecastConfig</code>","text":"<p>Configuration for payments forecasting.</p>"},{"location":"api-reference/forecasting/#attributes","title":"Attributes","text":"<ul> <li><code>horizon_days</code> (int): Number of days ahead to forecast (default: 7)</li> <li><code>metrics</code> (List[str]): List of metrics to forecast (default: cash, credit, debit, total)</li> <li><code>branches</code> (Optional[List[str]]): List of branch names to forecast. If None, infers from payments_df.</li> </ul>"},{"location":"api-reference/forecasting/#example","title":"Example","text":"<pre><code>from pos_core.forecasting import ForecastConfig\n\nconfig = ForecastConfig(\n    horizon_days=14,\n    metrics=[\"ingreso_efectivo\", \"ingreso_total\"],\n    branches=[\"Banana\", \"Queen\"]\n)\n</code></pre>"},{"location":"api-reference/forecasting/#forecastresult","title":"<code>ForecastResult</code>","text":"<p>Result dataclass containing forecast DataFrame, deposit schedule, and metadata.</p>"},{"location":"api-reference/forecasting/#attributes_1","title":"Attributes","text":"<ul> <li><code>forecast</code> (pd.DataFrame): DataFrame with columns: sucursal, fecha, metric, valor</li> <li><code>deposit_schedule</code> (pd.DataFrame): DataFrame with cash-flow deposit schedule</li> <li><code>metadata</code> (Dict[str, object]): Dictionary with additional metadata</li> <li><code>debug</code> (Optional[Dict[str, Dict[str, Dict[str, ModelDebugInfo]]]]): Optional nested dictionary of debug information. Only populated when <code>run_payments_forecast()</code> is called with <code>debug=True</code>. Structure: <code>debug[model_name][branch][metric] = ModelDebugInfo</code></li> </ul>"},{"location":"api-reference/forecasting/#metadata-keys","title":"Metadata Keys","text":"<ul> <li><code>branches</code>: List of branches forecasted</li> <li><code>metrics</code>: List of metrics forecasted</li> <li><code>horizon_days</code>: Number of days forecasted</li> <li><code>last_historical_date</code>: Last date in historical data</li> <li><code>successful_forecasts</code>: Number of successful forecasts</li> <li><code>failed_forecasts</code>: Number of failed forecasts</li> </ul>"},{"location":"api-reference/forecasting/#debug-information","title":"Debug Information","text":"<p>When <code>debug=True</code> is passed to <code>run_payments_forecast()</code>, the result includes model-specific debug information in <code>result.debug</code>. This allows you to inspect how each model generated its forecasts.</p> <p>Debug Structure: <pre><code>result.debug[model_name][branch][metric] = ModelDebugInfo\n</code></pre></p> <p>Example: <pre><code>result = run_payments_forecast(payments_df, config=config, debug=True)\n\n# Access debug info for naive model, Kavia branch, efectivo metric\nnaive_debug = result.debug[\"naive_last_week\"][\"Kavia\"][\"ingreso_efectivo\"]\nprint(naive_debug.model_name)  # \"naive_last_week\"\nprint(naive_debug.data[\"source_dates\"])  # Mapping of forecast dates to source dates\n</code></pre></p> <p>Each <code>ModelDebugInfo</code> contains: - <code>model_name</code> (str): Identifier for the model (e.g., \"naive_last_week\", \"arima\") - <code>version</code> (Optional[str]): Optional version string - <code>data</code> (Dict[str, Any]): Model-specific debug payload</p> <p>The <code>data</code> dictionary structure varies by model: - NaiveLastWeekModel: Contains <code>source_dates</code> mapping (forecast_date -&gt; source_date) - ARIMA models: Contains model parameters, AIC/BIC values, residuals, etc.</p>"},{"location":"api-reference/forecasting/#run_payments_forecast","title":"<code>run_payments_forecast()</code>","text":"<p>Main forecasting function.</p>"},{"location":"api-reference/forecasting/#signature","title":"Signature","text":"<pre><code>def run_payments_forecast(\n    payments_df: pd.DataFrame,\n    config: Optional[ForecastConfig] = None,\n    debug: bool = False,\n) -&gt; ForecastResult\n</code></pre>"},{"location":"api-reference/forecasting/#parameters","title":"Parameters","text":"<ul> <li><code>payments_df</code> (pd.DataFrame): Aggregated payments data, typically the output of the ETL step. Expected columns include at least:</li> <li><code>sucursal</code> (branch name)</li> <li><code>fecha</code> (date or datetime)</li> <li>the metrics in config.metrics (e.g. ingreso_efectivo, ingreso_credito, ...)</li> <li><code>config</code> (Optional[ForecastConfig]): ForecastConfig for horizon, metrics, and branches. If None, uses defaults.</li> <li><code>debug</code> (bool): If True, collects debug information from models and includes it in <code>result.debug</code>. Default is False to keep the API simple for normal use.</li> </ul>"},{"location":"api-reference/forecasting/#returns","title":"Returns","text":"<p>ForecastResult containing: - <code>forecast</code>: per-branch, per-metric predictions for the next horizon_days - <code>deposit_schedule</code>: computed cash-flow deposit schedule - <code>metadata</code>: additional information about the forecast - <code>debug</code>: model debug information (only if <code>debug=True</code>)</p>"},{"location":"api-reference/forecasting/#raises","title":"Raises","text":"<ul> <li><code>DataQualityError</code>: If required columns are missing, or no forecasts are generated.</li> </ul>"},{"location":"api-reference/forecasting/#example_1","title":"Example","text":"<pre><code>from pos_core.forecasting import ForecastConfig, run_payments_forecast\n\nconfig = ForecastConfig(horizon_days=7)\nresult = run_payments_forecast(payments_df, config=config)\n\nprint(result.forecast.head())\nprint(result.deposit_schedule)\n</code></pre>"},{"location":"api-reference/forecasting/#example-with-debug-information","title":"Example with Debug Information","text":"<pre><code>from pos_core.forecasting import ForecastConfig, run_payments_forecast\n\nconfig = ForecastConfig(horizon_days=7)\nresult = run_payments_forecast(payments_df, config=config, debug=True)\n\n# Access debug info for a specific model/branch/metric\nif result.debug:\n    naive_debug = result.debug[\"naive_last_week\"][\"Kavia\"][\"ingreso_efectivo\"]\n    source_dates = naive_debug.data[\"source_dates\"]\n    print(f\"Forecast dates mapped to source dates: {source_dates}\")\n</code></pre>"},{"location":"api-reference/forecasting/#modeldebuginfo","title":"<code>ModelDebugInfo</code>","text":"<p>Generic container for model-specific debug information.</p>"},{"location":"api-reference/forecasting/#attributes_2","title":"Attributes","text":"<ul> <li><code>model_name</code> (str): Short identifier for the model (e.g., \"naive_last_week\", \"arima\")</li> <li><code>version</code> (Optional[str]): Optional version string if model behavior changes over time</li> <li><code>data</code> (Dict[str, Any]): Model-specific payload (dict of JSON-like values)</li> </ul>"},{"location":"api-reference/forecasting/#model-specific-data-schemas","title":"Model-Specific Data Schemas","text":"<p>Each model populates <code>data</code> with its own schema:</p> <p>NaiveLastWeekModel: - <code>horizon_steps</code> (int): Number of forecast steps - <code>source_dates</code> (Dict[pd.Timestamp, pd.Timestamp]): Mapping of forecast dates to source historical dates</p> <p>ARIMA Models (future): - <code>order</code> (tuple): ARIMA order parameters - <code>aic</code> (float): Akaike Information Criterion - <code>bic</code> (float): Bayesian Information Criterion - <code>residuals_tail</code> (dict): Tail of residuals for inspection</p>"},{"location":"api-reference/forecasting/#accessing-debug-info","title":"Accessing Debug Info","text":"<p>Debug information can be accessed at two levels:</p> <ol> <li> <p>Model-level: Direct access to <code>model.debug_</code> attribute    <pre><code>from pos_core.forecasting.models.naive import NaiveLastWeekModel\n\nmodel = NaiveLastWeekModel()\nmodel_state = model.train(series)\nforecast = model.forecast(model_state, steps=7)\n\n# Access debug info directly from model\ndebug = model.debug_\nprint(debug.model_name)  # \"naive_last_week\"\nprint(debug.data[\"source_dates\"])\n</code></pre></p> </li> <li> <p>Pipeline-level: Through <code>ForecastResult.debug</code> (when <code>debug=True</code>)    <pre><code>result = run_payments_forecast(payments_df, config=config, debug=True)\ndebug = result.debug[\"naive_last_week\"][\"Kavia\"][\"ingreso_efectivo\"]\n</code></pre></p> </li> </ol>"},{"location":"api-reference/qa/","title":"QA API Reference","text":""},{"location":"api-reference/qa/#paymentsqaresult","title":"<code>PaymentsQAResult</code>","text":"<p>Result dataclass with QA summary and detailed findings.</p>"},{"location":"api-reference/qa/#attributes","title":"Attributes","text":"<ul> <li><code>summary</code> (dict): Dictionary with summary statistics and counts</li> <li><code>missing_days</code> (pd.DataFrame | None): DataFrame with missing days per sucursal, or None if none found</li> <li><code>duplicate_days</code> (pd.DataFrame | None): DataFrame with duplicate (sucursal, fecha) rows, or None if none found</li> <li><code>zscore_anomalies</code> (pd.DataFrame | None): DataFrame with z-score anomalies, or None if none found</li> <li><code>zero_method_flags</code> (pd.DataFrame | None): DataFrame with rows where tickets &gt; 0 but payment methods are zero, or None if none found</li> </ul>"},{"location":"api-reference/qa/#summary-keys","title":"Summary Keys","text":"<ul> <li><code>total_rows</code>: Total number of rows in the dataset</li> <li><code>total_sucursales</code>: Number of unique branches</li> <li><code>min_fecha</code>: Minimum date in the dataset</li> <li><code>max_fecha</code>: Maximum date in the dataset</li> <li><code>has_missing_days</code>: Boolean indicating if missing days were found</li> <li><code>has_duplicates</code>: Boolean indicating if duplicates were found</li> <li><code>has_zscore_anomalies</code>: Boolean indicating if z-score anomalies were found</li> <li><code>has_zero_method_flags</code>: Boolean indicating if zero method flags were found</li> <li><code>missing_days_count</code>: Number of missing days</li> <li><code>duplicate_days_count</code>: Number of duplicate days</li> <li><code>zscore_anomalies_count</code>: Number of z-score anomalies</li> <li><code>zero_method_flags_count</code>: Number of zero method flags</li> <li><code>schema_errors</code>: List of schema validation errors</li> </ul>"},{"location":"api-reference/qa/#run_payments_qa","title":"<code>run_payments_qa()</code>","text":"<p>Main QA function for data validation.</p>"},{"location":"api-reference/qa/#signature","title":"Signature","text":"<pre><code>def run_payments_qa(\n    payments_df: pd.DataFrame,\n    level: int = 4,\n) -&gt; PaymentsQAResult\n</code></pre>"},{"location":"api-reference/qa/#parameters","title":"Parameters","text":"<ul> <li><code>payments_df</code> (pd.DataFrame): Aggregated payments data, typically the output of the ETL step. Expected columns include at least:</li> <li><code>sucursal</code> (branch name)</li> <li><code>fecha</code> (date or datetime)</li> <li>payment method columns (ingreso_efectivo, ingreso_credito, etc.)</li> <li><code>num_tickets</code> (ticket count)</li> <li><code>level</code> (int): QA level to run (default: 4). Controls which checks are executed:</li> <li>Level 0: Schema validation (always run)</li> <li>Level 3: Missing and duplicate days</li> <li>Level 4: Statistical anomalies (z-score)</li> </ul>"},{"location":"api-reference/qa/#returns","title":"Returns","text":"<p>PaymentsQAResult containing: - <code>summary</code>: dictionary with counts and flags - <code>missing_days</code>: DataFrame with missing days per sucursal, or None - <code>duplicate_days</code>: DataFrame with duplicate rows, or None - <code>zscore_anomalies</code>: DataFrame with z-score anomalies, or None - <code>zero_method_flags</code>: DataFrame with zero method flags, or None</p>"},{"location":"api-reference/qa/#raises","title":"Raises","text":"<ul> <li><code>DataQualityError</code>: If required columns are missing.</li> </ul>"},{"location":"api-reference/qa/#example","title":"Example","text":"<pre><code>from pos_core.qa import run_payments_qa\n\nqa_result = run_payments_qa(payments_df, level=4)\n\nprint(f\"Missing days: {qa_result.summary['missing_days_count']}\")\nprint(f\"Anomalies: {qa_result.summary['zscore_anomalies_count']}\")\n\nif qa_result.missing_days is not None:\n    print(qa_result.missing_days)\n</code></pre>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/","title":"Documentation Structure Summary","text":""},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#final-structure","title":"\u2705 Final Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md                          # Landing page for docs site\n\u251c\u2500\u2500 user-guide/                       # User-facing documentation\n\u2502   \u251c\u2500\u2500 installation.md              # 1. Installation &amp; requirements\n\u2502   \u251c\u2500\u2500 quickstart.md                 # 2. Quick start guide\n\u2502   \u251c\u2500\u2500 examples.md                   # 3. Runnable examples (moved up)\n\u2502   \u251c\u2500\u2500 configuration.md              # 4. Configuration details\n\u2502   \u2514\u2500\u2500 concepts.md                   # 5. Concepts &amp; design decisions\n\u251c\u2500\u2500 api-reference/                    # API documentation\n\u2502   \u251c\u2500\u2500 etl.md                        # ETL API reference\n\u2502   \u251c\u2500\u2500 forecasting.md                # Forecasting API reference\n\u2502   \u251c\u2500\u2500 qa.md                         # QA API reference\n\u2502   \u2514\u2500\u2500 exceptions.md                 # Exceptions API reference\n\u2514\u2500\u2500 development/                      # Developer documentation (NEW)\n    \u2514\u2500\u2500 dev-notes.md                  # Development notes (moved here)\n\nRoot:\n\u251c\u2500\u2500 README.md                         # Main project README (GitHub)\n\u251c\u2500\u2500 CHANGELOG.md                      # Version history\n\u2514\u2500\u2500 DOCUMENTATION_REVIEW.md           # This review document\n\ntests/\n\u251c\u2500\u2500 README.md                         # Testing guide\n\u251c\u2500\u2500 LIVE_TESTS_QUICK_REFERENCE.md     # Quick reference\n\u2514\u2500\u2500 LIVE_TEST_SUMMARY.md              # Detailed summary\n\nexamples/\n\u2514\u2500\u2500 README.md                         # Examples guide\n</code></pre>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#improvements-made","title":"\ud83c\udfaf Improvements Made","text":""},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#1-fixed-orphaned-documentation","title":"1. Fixed Orphaned Documentation","text":"<ul> <li>\u2705 Moved <code>dev-notes.md</code> from <code>docs/</code> to <code>docs/development/</code></li> <li>\u2705 Added \"Development\" section to <code>mkdocs.yml</code> navigation</li> <li>\u2705 Developer documentation is now discoverable</li> </ul>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#2-optimized-navigation-order","title":"2. Optimized Navigation Order","text":"<ul> <li>\u2705 Reordered User Guide: Installation \u2192 Quickstart \u2192 Examples \u2192 Configuration \u2192 Concepts</li> <li>\u2705 Rationale: Examples come before detailed configuration (learn by doing)</li> <li>\u2705 More intuitive flow for new users</li> </ul>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#3-improved-cross-references","title":"3. Improved Cross-References","text":"<ul> <li>\u2705 Added \"Next Steps\" sections to guide users through documentation</li> <li>\u2705 Standardized link formats</li> <li>\u2705 Better navigation flow between pages</li> </ul>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#4-enhanced-user-experience","title":"4. Enhanced User Experience","text":"<ul> <li>\u2705 Clear progression: Install \u2192 Quickstart \u2192 Examples \u2192 Configure \u2192 Understand</li> <li>\u2705 Each page now has \"Next Steps\" to guide users</li> <li>\u2705 Consistent structure across all documentation</li> </ul>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#documentation-flow","title":"\ud83d\udcca Documentation Flow","text":""},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#for-new-users","title":"For New Users:","text":"<ol> <li>Installation \u2192 Install the package</li> <li>Quickstart \u2192 Get started quickly</li> <li>Examples \u2192 See working code</li> <li>Configuration \u2192 Customize setup</li> <li>Concepts \u2192 Understand design decisions</li> </ol>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#for-api-users","title":"For API Users:","text":"<ul> <li>API Reference \u2192 Complete API documentation</li> <li>ETL API</li> <li>Forecasting API</li> <li>QA API</li> <li>Exceptions</li> </ul>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#for-developers","title":"For Developers:","text":"<ul> <li>Development \u2192 Internal development notes</li> </ul>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#key-features","title":"\u2728 Key Features","text":"<ol> <li>Clear Separation: User guide vs API reference vs Development</li> <li>Logical Flow: Progressive disclosure from simple to complex</li> <li>Discoverable: All documentation accessible through navigation</li> <li>Cross-Referenced: Links guide users through the documentation</li> <li>Consistent: Uniform structure and formatting</li> </ol>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#navigation-structure-mkdocsyml","title":"\ud83d\udcdd Navigation Structure (mkdocs.yml)","text":"<pre><code>Home\n\u251c\u2500\u2500 User Guide\n\u2502   \u251c\u2500\u2500 Installation\n\u2502   \u251c\u2500\u2500 Quickstart\n\u2502   \u251c\u2500\u2500 Examples          \u2190 Moved up\n\u2502   \u251c\u2500\u2500 Configuration\n\u2502   \u2514\u2500\u2500 Concepts\n\u251c\u2500\u2500 API Reference\n\u2502   \u251c\u2500\u2500 ETL\n\u2502   \u251c\u2500\u2500 Forecasting\n\u2502   \u251c\u2500\u2500 QA\n\u2502   \u2514\u2500\u2500 Exceptions\n\u2514\u2500\u2500 Development           \u2190 NEW\n    \u2514\u2500\u2500 Development Notes\n</code></pre>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#best-practices-applied","title":"\ud83c\udfa8 Best Practices Applied","text":"<ol> <li>\u2705 Progressive Disclosure: Start simple, add complexity</li> <li>\u2705 Learn by Doing: Examples before detailed configuration</li> <li>\u2705 Clear Navigation: Logical order and cross-references</li> <li>\u2705 Separation of Concerns: User docs vs API vs Dev docs</li> <li>\u2705 Discoverability: All docs accessible through navigation</li> </ol>"},{"location":"development/DOCUMENTATION_STRUCTURE_SUMMARY/#notes","title":"\ud83d\udccc Notes","text":"<ul> <li>README.md serves GitHub visitors (different audience than docs site)</li> <li>docs/index.md serves documentation site visitors</li> <li>Some overlap is intentional and beneficial</li> <li>Test documentation stays in <code>tests/</code> directory (appropriate location)</li> <li>Examples documentation stays in <code>examples/</code> directory (appropriate location)</li> </ul>"},{"location":"development/dev-notes/","title":"Development Notes - ETL API Refactor","text":"<p>Note: This document contains internal development notes and is intended for contributors and maintainers. For user-facing documentation, see the User Guide and API Reference.</p>"},{"location":"development/dev-notes/#target-api","title":"Target API","text":"<p>The goal of this refactor is to create a clean, lean API that matches how we think about ETL workflows.</p>"},{"location":"development/dev-notes/#etl-configs","title":"ETL Configs","text":"<ul> <li><code>SalesETLConfig</code></li> <li><code>PaymentsETLConfig</code></li> </ul>"},{"location":"development/dev-notes/#stage-commands","title":"Stage Commands","text":"<ul> <li><code>download_sales(...)</code>, <code>clean_sales(...)</code>, <code>aggregate_sales(...)</code></li> <li><code>download_payments(...)</code>, <code>clean_payments(...)</code>, <code>aggregate_payments(...)</code></li> </ul>"},{"location":"development/dev-notes/#query-pandas-creator","title":"Query / Pandas Creator","text":"<ul> <li><code>get_sales(..., level=\"ticket|group|day\", refresh=False)</code></li> <li><code>get_payments(..., refresh=False)</code></li> <li><code>get_payments_forecast(..., horizon_weeks=13, refresh=False)</code></li> </ul> <p>Everything we build should support this set of calls.</p>"},{"location":"development/dev-notes/#constraints","title":"Constraints","text":"<p>To keep the codebase lean and prevent overbuilding:</p> <ol> <li>No new public function unless I have a real use for it right now.</li> <li>One concept \u2192 one way to do it.</li> <li>Public API uses simple types: strings, lists, DataFrames, dataclasses.</li> <li>If it feels too clever, don't do it.</li> </ol> <p>Check new code against this list before adding it.</p>"},{"location":"development/dev-notes/#current-public-api-inventory","title":"Current Public API Inventory","text":""},{"location":"development/dev-notes/#from-pos_coreetl","title":"From <code>pos_core.etl</code>","text":"<ul> <li><code>PaymentsETLConfig</code>, <code>PaymentsPaths</code></li> <li><code>build_payments_dataset</code></li> </ul>"},{"location":"development/dev-notes/#from-pos_coreforecasting","title":"From <code>pos_core.forecasting</code>","text":"<ul> <li><code>ForecastConfig</code>, <code>ForecastResult</code></li> <li><code>run_payments_forecast</code></li> </ul>"},{"location":"development/dev-notes/#from-pos_coreqa","title":"From <code>pos_core.qa</code>","text":"<ul> <li><code>PaymentsQAResult</code></li> <li><code>run_payments_qa</code></li> </ul>"},{"location":"development/dev-notes/#internal-modules","title":"Internal Modules","text":"<p>These are treated as internal (not re-exported from <code>__init__</code>): - <code>pos_core.etl.a_extract.*</code> - <code>pos_core.etl.b_transform.*</code> - <code>pos_core.etl.c_load.*</code> - Helper modules: <code>pos_core.etl.utils</code>, <code>pos_core.etl.branch_config</code></p>"},{"location":"development/dev-notes/#api-decisions","title":"API Decisions","text":""},{"location":"development/dev-notes/#keep-as-is","title":"Keep as-is","text":"<ul> <li><code>build_payments_dataset</code> - Will wrap internally later using stage functions</li> </ul>"},{"location":"development/dev-notes/#wrap","title":"Wrap","text":"<ul> <li>Will create <code>download_payments</code>, <code>clean_payments</code>, <code>aggregate_payments</code> that call existing internals</li> <li>Will create <code>download_sales</code>, <code>clean_sales</code>, <code>aggregate_sales</code> that call existing internals</li> </ul>"},{"location":"development/dev-notes/#mark-as-internal","title":"Mark as internal","text":"<ul> <li>All functions in <code>a_extract</code>, <code>b_transform</code>, <code>c_load</code> are already internal (not exported)</li> </ul>"},{"location":"development/dev-notes/#forecasting-model-debug-pattern","title":"Forecasting Model Debug Pattern","text":"<p>All forecasting models should implement a generic debug pattern to expose introspection information.</p>"},{"location":"development/dev-notes/#pattern-overview","title":"Pattern Overview","text":"<ul> <li>Generic entry point: Every model has a <code>.debug_</code> attribute of type <code>ModelDebugInfo | None</code></li> <li>Model-specific schema: Each model populates <code>ModelDebugInfo.data</code> with its own structure</li> <li>Pipeline integration: When <code>run_payments_forecast(debug=True)</code> is called, debug info is collected into <code>ForecastResult.debug</code></li> </ul>"},{"location":"development/dev-notes/#implementation-checklist","title":"Implementation Checklist","text":"<p>When adding a new forecasting model:</p> <ol> <li> <p>Add debug attribute to <code>__init__</code>: <pre><code>def __init__(self, ...) -&gt; None:\n    self.debug_: ModelDebugInfo | None = None\n</code></pre></p> </li> <li> <p>Populate debug info in <code>forecast()</code> method: <pre><code>def forecast(self, model: Any, steps: int, **kwargs: Any) -&gt; pd.Series:\n    # ... compute forecast ...\n    forecast_series = pd.Series(...)\n\n    self.debug_ = ModelDebugInfo(\n        model_name=\"your_model_name\",\n        version=\"v1\",  # optional\n        data={\n            # Model-specific fields\n        },\n    )\n\n    return forecast_series\n</code></pre></p> </li> <li> <p>Important constraints:</p> </li> <li>Never change the return type of <code>forecast()</code>; it always returns <code>pd.Series</code></li> <li>The <code>debug_</code> attribute should be set after computing the forecast</li> <li>Use <code>model_name</code> consistently (same string across all instances)</li> <li> <p>Keep <code>data</code> dict JSON-serializable if possible</p> </li> <li> <p>Debug info structure:</p> </li> <li>Debug info is stored in nested structure: <code>debug[model_name][branch][metric] = ModelDebugInfo</code></li> <li>Multiple models can coexist (e.g., <code>debug[\"naive_last_week\"]</code> and <code>debug[\"arima\"]</code>)</li> <li>Each branch/metric combination gets its own debug info instance</li> </ol> <p>See <code>src/pos_core/forecasting/models/__init__.py</code> for the complete checklist template.</p>"},{"location":"development/dev-notes/#periodic-lean-audit","title":"Periodic Lean Audit","text":"<p>Every few weeks, ask:</p> <ol> <li>Is there any public function nobody is using?</li> <li>Is there any duplicated logic between stage and query functions?</li> <li>Could two similar functions be merged into one with a simple parameter?</li> </ol>"},{"location":"development/new_api_plan/","title":"New API Design Plan","text":"<p>This document describes the clean, modern API structure for pos-core-etl.</p>"},{"location":"development/new_api_plan/#design-principles","title":"Design Principles","text":"<ol> <li>Domain-oriented modules: <code>pos_core.payments</code>, <code>pos_core.sales</code>, <code>pos_core.forecasting</code>, <code>pos_core.qa</code></li> <li>Clear grain semantics: Core facts at atomic grain, marts for aggregations</li> <li>Simple, opinionated API: One obvious way to do common tasks</li> <li>No backward compatibility cruft: Clean break from legacy naming</li> </ol>"},{"location":"development/new_api_plan/#module-structure","title":"Module Structure","text":"<pre><code>src/pos_core/\n\u251c\u2500\u2500 __init__.py              # Top-level exports (version, key functions)\n\u251c\u2500\u2500 config.py                # Unified configuration\n\u251c\u2500\u2500 exceptions.py            # Custom exceptions\n\u2502\n\u251c\u2500\u2500 payments/                # Payments domain\n\u2502   \u251c\u2500\u2500 __init__.py          # Public: get_payments()\n\u2502   \u251c\u2500\u2500 extract.py           # Bronze: download from Wansoft\n\u2502   \u251c\u2500\u2500 transform.py         # Silver: clean Excel \u2192 CSV (fact_payments_ticket)\n\u2502   \u2514\u2500\u2500 aggregate.py         # Gold: daily aggregation (mart_payments_daily)\n\u2502\n\u251c\u2500\u2500 sales/                   # Sales domain\n\u2502   \u251c\u2500\u2500 __init__.py          # Public: get_sales()\n\u2502   \u251c\u2500\u2500 extract.py           # Bronze: download from Wansoft\n\u2502   \u251c\u2500\u2500 transform.py         # Silver: clean Excel \u2192 CSV (fact_sales_item_line)\n\u2502   \u2514\u2500\u2500 aggregate.py         # Gold: ticket/group aggregations\n\u2502\n\u251c\u2500\u2500 transfers/               # Transfers domain\n\u2502   \u251c\u2500\u2500 __init__.py          # Public: raw, core, marts\n\u2502   \u251c\u2500\u2500 extract.py           # Bronze: download from Wansoft (Inventory &gt; Transfers &gt; Issued)\n\u2502   \u251c\u2500\u2500 transform.py         # Silver: clean Excel \u2192 CSV (fact_transfers_line)\n\u2502   \u2514\u2500\u2500 aggregate.py         # Gold: pivot aggregation (mart_transfers_pivot)\n\u2502\n\u251c\u2500\u2500 forecasting/             # Forecasting domain (mostly unchanged)\n\u2502   \u251c\u2500\u2500 __init__.py          # Public: run_payments_forecast, ForecastConfig, ForecastResult\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u2514\u2500\u2500 qa/                      # QA domain (mostly unchanged)\n    \u251c\u2500\u2500 __init__.py          # Public: run_payments_qa, QAResult\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"development/new_api_plan/#data-layers-and-grains","title":"Data Layers and Grains","text":""},{"location":"development/new_api_plan/#payments","title":"Payments","text":"<ul> <li>Bronze: <code>data/a_raw/payments/</code> - Raw Excel from Wansoft</li> <li>Silver (Core Fact): <code>data/b_clean/payments/</code> - <code>fact_payments_ticket</code></li> <li>Grain: ticket \u00d7 payment method</li> <li>Key: <code>(sucursal, operating_date, order_index, payment_method)</code></li> <li>Gold (Mart): <code>data/c_processed/payments/</code> - <code>mart_payments_daily</code></li> <li>Grain: sucursal \u00d7 date</li> <li>Aggregates: income by payment type, tips, ticket counts</li> </ul>"},{"location":"development/new_api_plan/#sales","title":"Sales","text":"<ul> <li>Bronze: <code>data/a_raw/sales/</code> - Raw Excel from Wansoft</li> <li>Silver (Core Fact): <code>data/b_clean/sales/</code> - <code>fact_sales_item_line</code></li> <li>Grain: item/modifier line</li> <li>Key: <code>(sucursal, operating_date, order_id, item_key, [modifier_cols])</code></li> <li>Gold (Marts): <code>data/c_processed/sales/</code></li> <li><code>mart_sales_by_ticket</code>: One row per ticket</li> <li><code>mart_sales_by_group</code>: Category pivot table</li> </ul>"},{"location":"development/new_api_plan/#transfers","title":"Transfers","text":"<ul> <li>Bronze: <code>data/a_raw/transfers/</code> - Raw Excel from Wansoft (Inventory &gt; Transfers &gt; Issued)</li> <li>Silver (Core Fact): <code>data/b_clean/transfers/</code> - <code>fact_transfers_line</code></li> <li>Grain: transfer line item</li> <li>Key: <code>(orden, almacen_origen, sucursal_destino, producto)</code></li> <li>Gold (Marts): <code>data/c_processed/transfers/</code></li> <li><code>mart_transfers_pivot</code>: Branch \u00d7 category pivot table</li> </ul>"},{"location":"development/new_api_plan/#public-api","title":"Public API","text":""},{"location":"development/new_api_plan/#configuration","title":"Configuration","text":"<pre><code>from pos_core import DataPaths\n\npaths = DataPaths.from_root(\n    data_root=\"data\",\n    sucursales_json=\"utils/sucursales.json\"\n)\n</code></pre>"},{"location":"development/new_api_plan/#payments_1","title":"Payments","text":"<pre><code>from pos_core.payments import get_payments\n\n# Get daily mart (default, most common use case)\ndf = get_payments(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Get core fact (ticket grain)\ndf = get_payments(paths, \"2025-01-01\", \"2025-01-31\", grain=\"ticket\")\n</code></pre>"},{"location":"development/new_api_plan/#sales_1","title":"Sales","text":"<pre><code>from pos_core.sales import get_sales\n\n# Get core fact (item-line grain, default)\ndf = get_sales(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Get ticket-level mart\ndf = get_sales(paths, \"2025-01-01\", \"2025-01-31\", grain=\"ticket\")\n\n# Get group pivot mart\ndf = get_sales(paths, \"2025-01-01\", \"2025-01-31\", grain=\"group\")\n</code></pre>"},{"location":"development/new_api_plan/#transfers_1","title":"Transfers","text":"<pre><code>from pos_core.transfers import core, marts\n\n# Get core fact (transfer line grain)\ndf = core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Get pivot mart (branch \u00d7 category)\ndf = marts.fetch_pivot(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"development/new_api_plan/#forecasting","title":"Forecasting","text":"<pre><code>from pos_core.payments import get_payments\nfrom pos_core.forecasting import run_payments_forecast, ForecastConfig\n\n# Get historical data\npayments_df = get_payments(paths, \"2022-01-01\", \"2025-01-31\")\n\n# Run forecast\nresult = run_payments_forecast(payments_df, ForecastConfig(horizon_days=91))\nprint(result.forecast)\n</code></pre>"},{"location":"development/new_api_plan/#qa","title":"QA","text":"<pre><code>from pos_core.payments import get_payments\nfrom pos_core.qa import run_payments_qa\n\ndf = get_payments(paths, \"2025-01-01\", \"2025-01-31\")\nresult = run_payments_qa(df)\n</code></pre>"},{"location":"development/new_api_plan/#removedchanged-from-old-api","title":"Removed/Changed from Old API","text":"Old New Notes <code>PaymentsETLConfig</code> <code>DataPaths</code> Simplified, unified <code>SalesETLConfig</code> <code>DataPaths</code> Same config for both <code>get_sales(level=\"ticket\")</code> <code>get_sales(grain=\"ticket\")</code> Clearer parameter name <code>get_payments_forecast()</code> Use <code>get_payments()</code> + <code>run_payments_forecast()</code> Explicit composition <code>build_payments_dataset()</code> <code>get_payments()</code> Simplified <code>download_payments</code>, <code>clean_payments</code>, <code>aggregate_payments</code> Internal Not part of public API"},{"location":"development/new_api_plan/#migration-notes","title":"Migration Notes","text":"<p>The API has been simplified. Old entry points were intentionally removed to reduce technical debt. The new API: - Uses a unified <code>DataPaths</code> config for both payments and sales - Uses <code>grain=</code> parameter instead of <code>level=</code> for clarity - Exposes core facts and marts through the same <code>get_*</code> functions - Keeps ETL implementation details internal</p>"},{"location":"user-guide/concepts/","title":"Concepts","text":"<p>This page explains key concepts and design decisions in POS Core ETL.</p>"},{"location":"user-guide/concepts/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Data Layers</li> <li>Data Grains</li> <li>API Design</li> <li>Branch Code Windows</li> <li>Metadata and Idempotence</li> <li>POS System Requirements</li> <li>Forecasting Model</li> </ul>"},{"location":"user-guide/concepts/#data-layers-bronzesilvergold","title":"Data Layers (Bronze/Silver/Gold)","text":"<p>The package follows industry-standard data engineering conventions with explicit data layers:</p> Layer Description Data Directory Format Bronze Raw Wansoft exports, unchanged <code>data/a_raw/</code> Excel files Silver (Core) Core facts at atomic grain <code>data/b_clean/</code> CSV files Gold (Marts) Aggregated tables for analysis <code>data/c_processed/</code> CSV files"},{"location":"user-guide/concepts/#directory-convention","title":"Directory Convention","text":"<ul> <li><code>a_raw/</code>: Bronze - Data files downloaded from POS API (Excel files)</li> <li><code>b_clean/</code>: Silver - Core facts at atomic grain (CSV files)</li> <li><code>c_processed/</code>: Gold - Marts (aggregated datasets)</li> </ul> <p>This convention makes it easy to: - Identify which layer each file belongs to - Re-run specific stages without re-processing everything - Debug issues at each layer</p>"},{"location":"user-guide/concepts/#layer-flow","title":"Layer Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Bronze    \u2502 \u2500\u2500\u25b6 \u2502    Silver (Core)        \u2502 \u2500\u2500\u25b6 \u2502   Gold (Marts)  \u2502\n\u2502             \u2502     \u2502                         \u2502     \u2502                 \u2502\n\u2502 a_raw/      \u2502     \u2502 b_clean/                \u2502     \u2502 c_processed/    \u2502\n\u2502 Excel files \u2502     \u2502 \u2022 fact_payments_ticket  \u2502     \u2502 \u2022 By ticket     \u2502\n\u2502             \u2502     \u2502 \u2022 fact_sales_item_line  \u2502     \u2502 \u2022 By day        \u2502\n\u2502             \u2502     \u2502 \u2022 fact_transfers_line   \u2502     \u2502 \u2022 By category   \u2502\n\u2502             \u2502     \u2502                         \u2502     \u2502 \u2022 Pivot tables  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/#data-grains","title":"Data Grains","text":"<p>Understanding data grain is essential for working with this package. Each domain has a specific atomic grain that defines the most granular meaningful unit of data.</p>"},{"location":"user-guide/concepts/#grain-definitions","title":"Grain Definitions","text":"Domain Core Fact Grain Key Payments <code>fact_payments_ticket</code> ticket \u00d7 payment method <code>(sucursal, operating_date, order_index, payment_method)</code> Sales <code>fact_sales_item_line</code> item/modifier line <code>(sucursal, operating_date, order_id, item_key)</code> Transfers <code>fact_transfers_line</code> transfer line item <code>(orden, almacen_origen, sucursal_destino, producto)</code>"},{"location":"user-guide/concepts/#key-rule","title":"Key Rule","text":"<ul> <li> <p>Sales: The most granular meaningful unit is item/modifier line. Anything aggregated beyond this (ticket-level, day-level, group-level) is a mart, not core.</p> </li> <li> <p>Payments: The most granular meaningful unit is ticket \u00d7 payment method. This IS the atomic fact, so it sits in the silver/core layer.</p> </li> </ul>"},{"location":"user-guide/concepts/#example-sales-data-flow","title":"Example: Sales Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Raw (Bronze): Excel file with sales transactions                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Silver (Core) = Core Fact: fact_sales_item_line                            \u2502\n\u2502                                                                             \u2502\n\u2502 Grain: One row per item/modifier line on a ticket                          \u2502\n\u2502 Key: (sucursal, operating_date, order_id, item_key)                        \u2502\n\u2502                                                                             \u2502\n\u2502 Example rows for ticket #1001:                                              \u2502\n\u2502   Row 1: Caf\u00e9 Americano (item_key=CAFE01, group=CAFE)                       \u2502\n\u2502   Row 2: Pan Dulce (item_key=PAN01, group=PAN DULCE)                        \u2502\n\u2502   Row 3: Extra Leche (item_key=MOD01, is_modifier=True)                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Gold (Marts): Aggregations built from core fact                            \u2502\n\u2502                                                                             \u2502\n\u2502 \u2022 mart_sales_by_ticket: One row per ticket (aggregates item-lines)         \u2502\n\u2502 \u2022 mart_sales_by_group: Category pivot tables (aggregates by group)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/#example-payments-data-flow","title":"Example: Payments Data Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Raw (Bronze): Excel file with payment transactions                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Silver (Core) = Core Fact: fact_payments_ticket                            \u2502\n\u2502                                                                             \u2502\n\u2502 Grain: One row per ticket \u00d7 payment method                                 \u2502\n\u2502 Key: (sucursal, operating_date, order_index, payment_method)               \u2502\n\u2502                                                                             \u2502\n\u2502 Example rows for a split payment (ticket #1001):                            \u2502\n\u2502   Row 1: order_index=1001, payment_method=Efectivo, ticket_total=50.00     \u2502\n\u2502   Row 2: order_index=1001, payment_method=Tarjeta Cr\u00e9dito, ticket_total=100\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Gold (Marts): Aggregations built from core fact                            \u2502\n\u2502                                                                             \u2502\n\u2502 \u2022 mart_payments_daily: One row per sucursal \u00d7 day                          \u2502\n\u2502   Columns: ingreso_efectivo, ingreso_credito, propinas, num_tickets, etc.  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/concepts/#why-this-matters","title":"Why This Matters","text":"<ol> <li>Data Integrity: Understanding the grain ensures you're not accidentally double-counting or losing data.</li> <li>Correct Joins: When joining tables, you need to understand the grain to choose the right join keys.</li> <li>Query Efficiency: Querying at the right grain level improves performance and accuracy.</li> <li>Debugging: When data doesn't match expectations, checking the grain is often the first step.</li> </ol>"},{"location":"user-guide/concepts/#api-design","title":"API Design","text":"<p>The package uses domain + layer modules to encode specificity:</p>"},{"location":"user-guide/concepts/#module-structure","title":"Module Structure","text":"<ul> <li>Modules define domain + layer:</li> <li><code>pos_core.payments.core</code> \u2192 payments, silver (core fact)</li> <li><code>pos_core.payments.marts</code> \u2192 payments, gold (aggregates)</li> <li><code>pos_core.payments.raw</code> \u2192 payments, bronze (extraction)</li> <li><code>pos_core.sales.core</code> \u2192 sales, silver</li> <li><code>pos_core.sales.marts</code> \u2192 sales, gold</li> <li><code>pos_core.sales.raw</code> \u2192 sales, bronze</li> <li><code>pos_core.transfers.core</code> \u2192 transfers, silver (core fact)</li> <li><code>pos_core.transfers.marts</code> \u2192 transfers, gold (pivot table)</li> <li><code>pos_core.transfers.raw</code> \u2192 transfers, bronze (extraction)</li> <li> <p><code>pos_core.order_times.raw</code> \u2192 order times, bronze (extraction)</p> </li> <li> <p>Functions define behavior:</p> </li> <li><code>fetch(...)</code> / <code>fetch_*</code>: Run ETL for missing partitions (or all if <code>mode=\"force\"</code>)</li> <li><code>load(...)</code> / <code>load_*</code>: Read existing outputs only; never run ETL</li> </ul>"},{"location":"user-guide/concepts/#configuration","title":"Configuration","text":"<pre><code>from pathlib import Path\nfrom pos_core import DataPaths\n\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n</code></pre>"},{"location":"user-guide/concepts/#payments-api","title":"Payments API","text":"<pre><code>from pos_core.payments import core, marts\n\n# Get daily mart (most common use case)\ndaily_df = marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Get core fact (ticket \u00d7 payment method grain)\nfact_df = core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Load existing data only (faster, but requires data to exist)\ndaily_df = marts.load_daily(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"user-guide/concepts/#sales-api","title":"Sales API","text":"<pre><code>from pos_core.sales import core, marts\n\n# Get core fact (item-line grain)\nfact_df = core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Get ticket mart\nticket_df = marts.fetch_ticket(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Get group mart\ngroup_df = marts.fetch_group(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"user-guide/concepts/#transfers-api","title":"Transfers API","text":"<pre><code>from pos_core.transfers import core, marts\n\n# Get core fact (transfer line grain)\nfact_df = core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Get pivot mart (branch \u00d7 category aggregation)\npivot_df = marts.fetch_pivot(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Include CEDIS as destination (default is False)\npivot_df = marts.fetch_pivot(paths, \"2025-01-01\", \"2025-01-31\", include_cedis=True)\n</code></pre>"},{"location":"user-guide/concepts/#order-times-api","title":"Order Times API","text":"<pre><code>from pos_core.order_times import raw\n\n# Download raw order times data\nraw.fetch(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Verify data exists\nraw.load(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"user-guide/concepts/#processing-modes","title":"Processing Modes","text":"<ul> <li><code>mode=\"missing\"</code> (default): Only runs ETL for date ranges that don't have completed outputs</li> <li><code>mode=\"force\"</code>: Forces re-run of all ETL stages for the given date range</li> </ul> <pre><code># Default: skip if data exists\ndf = marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Force refresh\ndf = marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\", mode=\"force\")\n</code></pre>"},{"location":"user-guide/concepts/#branch-code-windows","title":"Branch Code Windows","text":"<p>Branches (sucursales) may change their codes over time. The package handles this through \"validity windows\" in <code>sucursales.json</code>.</p> <p>Each branch entry can specify: - <code>code</code>: The POS system code for this branch - <code>valid_from</code>: When this code became active (YYYY-MM-DD format) - <code>valid_to</code>: When this code became inactive (null if still active)</p> <p>Example: If a branch changed codes on 2024-06-01:</p> <pre><code>{\n  \"MyBranch\": {\n    \"code\": \"5678\",\n    \"valid_from\": \"2024-06-01\",\n    \"valid_to\": null\n  },\n  \"MyBranch_OLD\": {\n    \"code\": \"1234\",\n    \"valid_from\": \"2020-01-01\",\n    \"valid_to\": \"2024-05-31\"\n  }\n}\n</code></pre> <p>Branch codes are resolved using <code>pos_core.branches.BranchRegistry</code>:</p> <pre><code>from pos_core.branches import BranchRegistry\n\nregistry = BranchRegistry(paths)\n\n# Get code for a specific date\ncode = registry.get_code_for_date(\"MyBranch\", \"2023-01-15\")  # Returns \"1234\"\ncode = registry.get_code_for_date(\"MyBranch\", \"2024-07-01\")  # Returns \"5678\"\n</code></pre>"},{"location":"user-guide/concepts/#metadata-and-idempotence","title":"Metadata and Idempotence","text":"<p>The ETL pipeline uses metadata files to track stage completion and enable idempotent operations.</p>"},{"location":"user-guide/concepts/#metadata-storage","title":"Metadata Storage","text":"<p>Metadata files are stored in <code>_meta/</code> subdirectories within each stage directory: - <code>a_raw/payments/batch/_meta/2025-01-01_2025-01-31.json</code> - <code>b_clean/payments/batch/_meta/2025-01-01_2025-01-31.json</code> - <code>c_processed/payments/_meta/2025-01-01_2025-01-31.json</code></p>"},{"location":"user-guide/concepts/#automatic-idempotence","title":"Automatic Idempotence","text":"<p><code>fetch()</code> functions automatically check metadata: - If metadata exists and <code>status == \"ok\"</code>, skip the stage - If <code>mode=\"force\"</code>, force re-run all stages - If <code>mode=\"missing\"</code> (default), use existing data when available</p> <p>This makes it safe to re-run queries without duplicating work.</p>"},{"location":"user-guide/concepts/#fetch-vs-load","title":"Fetch vs Load","text":"<ul> <li><code>fetch()</code>: May run ETL if data doesn't exist or if <code>mode=\"force\"</code></li> <li><code>load()</code>: Never runs ETL; only reads existing data (raises error if missing)</li> </ul> <pre><code># Fetch: runs ETL if needed\ndf = marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Load: read only (faster, but requires existing data)\ndf = marts.load_daily(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"user-guide/concepts/#pos-system-requirements","title":"POS System Requirements","text":"<p>This package is designed for POS systems that:</p> <ol> <li>Expose HTTP exports for payments, sales detail, and transfer reports</li> <li>Use Excel format for exported reports</li> <li>Support authentication via username/password (required for downloading raw data; optional if you already have raw data files)</li> </ol> <p>The package is optimized for Wansoft-style POS systems.</p>"},{"location":"user-guide/concepts/#forecasting-model","title":"Forecasting Model","text":"<p>The forecasting module uses ARIMA (AutoRegressive Integrated Moving Average) models:</p> <ul> <li>Log transformation: Applied to handle non-negative values</li> <li>Automatic hyperparameter selection: Searches for optimal parameters</li> <li>Per-branch, per-metric: Separate models for each combination</li> <li>Fallback models: Uses naive models when ARIMA fails</li> </ul> <p>Models require at least 30 days of historical data for reliable forecasts.</p>"},{"location":"user-guide/concepts/#usage","title":"Usage","text":"<pre><code>from pos_core.payments import marts as payments_marts\nfrom pos_core.forecasting import ForecastConfig, run_payments_forecast\n\n# Get historical data\npayments_df = payments_marts.fetch_daily(paths, \"2022-01-01\", \"2025-01-31\")\n\n# Run forecast\nconfig = ForecastConfig(horizon_days=91)  # 13 weeks\nresult = run_payments_forecast(payments_df, config)\n\n# Access results\nprint(result.forecast.head())  # Per-branch/metric forecasts\nprint(result.deposit_schedule.head())  # Cash-flow deposits\n</code></pre>"},{"location":"user-guide/concepts/#migration-from-v02x","title":"Migration from v0.2.x","text":"<p>The API was refactored in v0.3.0 to use module namespaces. Key changes:</p> Old API (v0.2.x) New API (v0.3.x) <code>from pos_core.payments import get_payments</code> <code>from pos_core.payments import core, marts</code> <code>get_payments(..., grain=\"ticket\")</code> <code>payments.core.fetch(...)</code> <code>get_payments(..., grain=\"daily\")</code> <code>payments.marts.fetch_daily(...)</code> <code>get_payments(..., refresh=True)</code> <code>payments.core.fetch(..., mode=\"force\")</code> <code>from pos_core.sales import get_sales</code> <code>from pos_core.sales import core, marts</code> <code>get_sales(..., grain=\"item\")</code> <code>sales.core.fetch(...)</code> <code>get_sales(..., grain=\"ticket\")</code> <code>sales.marts.fetch_ticket(...)</code> <code>get_sales(..., grain=\"group\")</code> <code>sales.marts.fetch_group(...)</code> <p>The new API: - Uses module namespaces (<code>payments.core</code>, <code>payments.marts</code>, <code>sales.core</code>, <code>sales.marts</code>) to encode domain + layer - Uses short verb-based names (<code>fetch</code>, <code>load</code>) for behavior - Makes bronze/silver/gold layers explicit via module paths - Supports <code>mode=\"missing\"</code> (default) vs <code>mode=\"force\"</code> for partition-aware ETL - Provides <code>load()</code> functions that never run ETL (read-only)</p>"},{"location":"user-guide/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Try Examples - Complete runnable example scripts</li> <li>API Reference - Detailed function documentation</li> <li>Quickstart - Get started in minutes</li> </ul>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>This guide covers all configuration options for POS Core ETL, including branch configuration, environment variables, and data paths.</p>"},{"location":"user-guide/configuration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Branch Configuration</li> <li>Environment Variables</li> <li>Data Paths Configuration</li> <li>Forecasting Configuration</li> </ul>"},{"location":"user-guide/configuration/#branch-configuration-sucursalesjson","title":"Branch Configuration (sucursales.json)","text":"<p>The <code>sucursales.json</code> file maps branch names to codes and tracks validity windows for branches that change codes over time.</p>"},{"location":"user-guide/configuration/#file-location","title":"File Location","text":"<p>Default location: <code>utils/sucursales.json</code> (relative to your project root)</p> <p>You can specify a custom location when creating <code>DataPaths</code>:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\n\npaths = DataPaths.from_root(\n    Path(\"data\"),\n    Path(\"custom/path/sucursales.json\")  # Custom location\n)\n</code></pre>"},{"location":"user-guide/configuration/#file-structure","title":"File Structure","text":"<p>Example structure:</p> <pre><code>{\n  \"Banana\": {\n    \"code\": \"8888\",\n    \"valid_from\": \"2024-02-21\",\n    \"valid_to\": null\n  },\n  \"Queen\": {\n    \"code\": \"6362\",\n    \"valid_from\": \"2024-01-01\",\n    \"valid_to\": null\n  },\n  \"Kavia_OLD\": {\n    \"code\": \"6161\",\n    \"valid_from\": \"2022-11-01\",\n    \"valid_to\": \"2024-02-20\"\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#fields","title":"Fields","text":"<ul> <li><code>code</code> (string, required): Branch code used by the POS system (e.g., \"8888\")</li> <li><code>valid_from</code> (string, required): Date when this code became active (YYYY-MM-DD format)</li> <li><code>valid_to</code> (string | null): Date when this code became inactive (null if still active)</li> </ul>"},{"location":"user-guide/configuration/#branch-code-windows","title":"Branch Code Windows","text":"<p>Branches may change their codes over time. The package handles this through validity windows:</p> <pre><code>{\n  \"MyBranch\": {\n    \"code\": \"5678\",\n    \"valid_from\": \"2024-06-01\",\n    \"valid_to\": null\n  },\n  \"MyBranch_OLD\": {\n    \"code\": \"1234\",\n    \"valid_from\": \"2020-01-01\",\n    \"valid_to\": \"2024-05-31\"\n  }\n}\n</code></pre> <p>The <code>BranchRegistry</code> automatically resolves the correct code for a given date:</p> <pre><code>from pos_core.branches import BranchRegistry\n\nregistry = BranchRegistry(paths)\n\n# Get code for a specific date\ncode = registry.get_code_for_date(\"MyBranch\", \"2023-01-15\")  # Returns \"1234\"\ncode = registry.get_code_for_date(\"MyBranch\", \"2024-07-01\")  # Returns \"5678\"\n</code></pre>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"user-guide/configuration/#required-for-extraction","title":"Required for Extraction","text":"<p>Required for downloading raw data from the POS API:</p> <ul> <li><code>WS_BASE</code> (required): Base URL of your POS instance</li> <li><code>WS_USER</code> (required): Username for authentication</li> <li><code>WS_PASS</code> (required): Password for authentication</li> </ul> <p>Example (bash): <pre><code>export WS_BASE=\"https://your-pos-instance.com\"\nexport WS_USER=\"your_username\"\nexport WS_PASS=\"your_password\"\n</code></pre></p> <p>Example (PowerShell): <pre><code>$env:WS_BASE = \"https://your-pos-instance.com\"\n$env:WS_USER = \"your_username\"\n$env:WS_PASS = \"your_password\"\n</code></pre></p> <p>Example (Python): <pre><code>import os\nos.environ[\"WS_BASE\"] = \"https://your-pos-instance.com\"\nos.environ[\"WS_USER\"] = \"your_username\"\nos.environ[\"WS_PASS\"] = \"your_password\"\n</code></pre></p>"},{"location":"user-guide/configuration/#when-environment-variables-are-not-needed","title":"When Environment Variables Are Not Needed","text":"<p>If you only work with already-downloaded files in <code>a_raw/</code>, these environment variables are not needed. The package will skip extraction and work directly with existing raw data files.</p>"},{"location":"user-guide/configuration/#security-best-practices","title":"Security Best Practices","text":"<p>Never commit secrets or sensitive data to version control.</p> <ul> <li>Store environment variables in <code>.env</code> files (add to <code>.gitignore</code>)</li> <li>Use environment variable management tools in production</li> <li>Never hardcode credentials in your code</li> </ul>"},{"location":"user-guide/configuration/#data-paths-configuration","title":"Data Paths Configuration","text":""},{"location":"user-guide/configuration/#default-configuration","title":"Default Configuration","text":"<p>The simplest way to configure paths is using <code>DataPaths.from_root()</code>:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\n\npaths = DataPaths.from_root(\n    data_root=Path(\"data\"),\n    sucursales_file=Path(\"utils/sucursales.json\")\n)\n</code></pre> <p>This creates a standard directory structure:</p> <pre><code>data/\n\u251c\u2500\u2500 a_raw/          # Bronze: Raw Wansoft exports\n\u2502   \u251c\u2500\u2500 payments/\n\u2502   \u2514\u2500\u2500 sales/\n\u251c\u2500\u2500 b_clean/        # Silver: Core facts\n\u2502   \u251c\u2500\u2500 payments/\n\u2502   \u2514\u2500\u2500 sales/\n\u2514\u2500\u2500 c_processed/    # Gold: Marts\n    \u251c\u2500\u2500 payments/\n    \u2514\u2500\u2500 sales/\n</code></pre>"},{"location":"user-guide/configuration/#custom-paths","title":"Custom Paths","text":"<p>For advanced use cases, you can create custom paths:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\n\n# Create custom DataPaths\npaths = DataPaths(\n    raw_payments=Path(\"custom/raw/payments\"),\n    raw_sales=Path(\"custom/raw/sales\"),\n    clean_payments=Path(\"custom/clean/payments\"),\n    clean_sales=Path(\"custom/clean/sales\"),\n    mart_payments=Path(\"custom/marts/payments\"),\n    mart_sales=Path(\"custom/marts/sales\"),\n    sucursales_json=Path(\"custom/sucursales.json\")\n)\n</code></pre>"},{"location":"user-guide/configuration/#datapaths-attributes","title":"DataPaths Attributes","text":"<p>The <code>DataPaths</code> object provides access to all path configurations:</p> <ul> <li><code>raw_payments</code> (Path): Directory for raw payment Excel files</li> <li><code>raw_sales</code> (Path): Directory for raw sales Excel files</li> <li><code>raw_order_times</code> (Path): Directory for raw order times Excel files</li> <li><code>clean_payments</code> (Path): Directory for cleaned payment CSV files</li> <li><code>clean_sales</code> (Path): Directory for cleaned sales CSV files</li> <li><code>clean_order_times</code> (Path): Directory for cleaned order times CSV files</li> <li><code>mart_payments</code> (Path): Directory for payment marts</li> <li><code>mart_sales</code> (Path): Directory for sales marts</li> <li><code>mart_order_times</code> (Path): Directory for order times marts</li> <li><code>sucursales_json</code> (Path): Path to <code>sucursales.json</code> file</li> </ul>"},{"location":"user-guide/configuration/#ensuring-directories-exist","title":"Ensuring Directories Exist","text":"<p>The <code>DataPaths</code> object can automatically create directories:</p> <pre><code>paths.ensure_dirs()  # Creates all directories if they don't exist\n</code></pre> <p>This is automatically called by <code>fetch()</code> functions, but you can call it manually if needed.</p>"},{"location":"user-guide/configuration/#directory-structure","title":"Directory Structure","text":"<p>The package follows a bronze/silver/gold data layer convention:</p> <pre><code>data/\n\u251c\u2500\u2500 a_raw/          # Bronze: Raw Wansoft exports (Excel files)\n\u2502   \u251c\u2500\u2500 payments/\n\u2502   \u2502   \u2514\u2500\u2500 batch/  # Date-partitioned raw files\n\u2502   \u251c\u2500\u2500 sales/\n\u2502   \u2502   \u2514\u2500\u2500 batch/  # Date-partitioned raw files\n\u2502   \u2514\u2500\u2500 order_times/\n\u2502       \u2514\u2500\u2500 batch/  # Date-partitioned raw files\n\u251c\u2500\u2500 b_clean/        # Silver: Core facts at atomic grain (CSV files)\n\u2502   \u251c\u2500\u2500 payments/\n\u2502   \u2502   \u2514\u2500\u2500 batch/  # Date-partitioned core fact files\n\u2502   \u251c\u2500\u2500 sales/\n\u2502   \u2502   \u2514\u2500\u2500 batch/  # Date-partitioned core fact files\n\u2502   \u2514\u2500\u2500 order_times/\n\u2502       \u2514\u2500\u2500 batch/  # Date-partitioned core fact files\n\u2514\u2500\u2500 c_processed/    # Gold: Marts (aggregated tables)\n    \u251c\u2500\u2500 payments/\n    \u2502   \u2514\u2500\u2500 _meta/  # Metadata for marts\n    \u251c\u2500\u2500 sales/\n    \u2514\u2500\u2500 order_times/\n        \u2514\u2500\u2500 _meta/  # Metadata for marts\n</code></pre>"},{"location":"user-guide/configuration/#metadata-files","title":"Metadata Files","text":"<p>Metadata files are stored in <code>_meta/</code> subdirectories to track ETL stage completion:</p> <ul> <li><code>a_raw/payments/batch/_meta/2025-01-01_2025-01-31.json</code></li> <li><code>b_clean/payments/batch/_meta/2025-01-01_2025-01-31.json</code></li> <li><code>c_processed/payments/_meta/2025-01-01_2025-01-31.json</code></li> </ul> <p>These files enable idempotent operations - the package automatically skips work that's already been done.</p>"},{"location":"user-guide/configuration/#forecasting-configuration","title":"Forecasting Configuration","text":""},{"location":"user-guide/configuration/#forecastconfig","title":"ForecastConfig","text":"<p>Configure forecasting behavior:</p> <pre><code>from pos_core.forecasting import ForecastConfig\n\n# Default configuration (7 days, all metrics, all branches)\nconfig = ForecastConfig()\n\n# Custom configuration\nconfig = ForecastConfig(\n    horizon_days=91,  # 13 weeks\n    metrics=[\"ingreso_efectivo\", \"ingreso_total\"],  # Specific metrics\n    branches=[\"Banana\", \"Queen\"]  # Specific branches\n)\n</code></pre>"},{"location":"user-guide/configuration/#parameters","title":"Parameters","text":"<ul> <li><code>horizon_days</code> (int, default: 7): Number of days ahead to forecast</li> <li><code>metrics</code> (list[str], default: all available): List of metrics to forecast</li> <li>Common metrics: <code>\"ingreso_efectivo\"</code>, <code>\"ingreso_credito\"</code>, <code>\"ingreso_debito\"</code>, <code>\"ingreso_total\"</code></li> <li><code>branches</code> (list[str] | None, default: None): List of branch names to forecast</li> <li>If None, forecasts all branches found in the data</li> </ul>"},{"location":"user-guide/configuration/#example","title":"Example","text":"<pre><code>from pos_core.forecasting import ForecastConfig, run_payments_forecast\nfrom pos_core.payments import marts as payments_marts\n\n# Get historical data\npayments_df = payments_marts.fetch_daily(paths, \"2022-01-01\", \"2025-01-31\")\n\n# Configure forecast\nconfig = ForecastConfig(\n    horizon_days=91,  # 13 weeks\n    metrics=[\"ingreso_efectivo\", \"ingreso_total\"]\n)\n\n# Run forecast\nresult = run_payments_forecast(payments_df, config)\n</code></pre>"},{"location":"user-guide/configuration/#qa-configuration","title":"QA Configuration","text":""},{"location":"user-guide/configuration/#qa-levels","title":"QA Levels","text":"<p>The QA module supports different levels of checks:</p> <pre><code>from pos_core.qa import run_payments_qa\n\n# Level 0: Schema validation (always run)\nresult = run_payments_qa(df, level=0)\n\n# Level 3: Missing and duplicate days\nresult = run_payments_qa(df, level=3)\n\n# Level 4: Statistical anomalies (z-score) - default\nresult = run_payments_qa(df, level=4)\n</code></pre> <p>QA Levels: - Level 0: Schema validation (always run) - Level 3: Missing days and duplicate detection - Level 4: Statistical anomalies (z-score analysis)</p>"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Get started with a working example</li> <li>Concepts - Understand data layers, grains, and API design</li> <li>Examples - Complete runnable example scripts</li> <li>API Reference - Detailed function documentation</li> </ul>"},{"location":"user-guide/examples/","title":"Examples","text":"<p>This page provides complete runnable example scripts demonstrating POS Core ETL usage. All examples are self-contained and can be run directly.</p>"},{"location":"user-guide/examples/#prerequisites","title":"Prerequisites","text":"<p>Before running any example:</p> <ol> <li>Install the package: <code>pip install -e .</code> (or <code>pip install pos-core-etl</code> for production)</li> <li>Create <code>utils/sucursales.json</code>: Branch configuration file (see Configuration)</li> <li>Set environment variables (required for online extraction):</li> <li><code>WS_BASE</code>: Base URL of your POS instance</li> <li><code>WS_USER</code>: Username for authentication</li> <li><code>WS_PASS</code>: Password for authentication</li> </ol> <p>Example: <pre><code>export WS_BASE=\"https://your-pos-instance.com\"\nexport WS_USER=\"your_username\"\nexport WS_PASS=\"your_password\"\n</code></pre> 4. Create data directory structure (or modify paths in scripts):    <pre><code>data/\n\u251c\u2500\u2500 a_raw/\n\u251c\u2500\u2500 b_clean/\n\u2514\u2500\u2500 c_processed/\n</code></pre></p>"},{"location":"user-guide/examples/#example-1-payments-daily-mart","title":"Example 1: Payments Daily Mart","text":"<p>File: <code>examples/payments_full_etl.py</code></p> <p>Demonstrates fetching payments data at the daily mart level:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import marts as payments_marts\n\n# Configure paths\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Fetch daily mart (most common use case)\n# This automatically handles extraction, transformation, and aggregation\npayments_daily = payments_marts.fetch_daily(\n    paths, \n    \"2025-01-01\", \n    \"2025-01-31\"\n)\n\nprint(f\"Retrieved {len(payments_daily)} rows\")\nprint(payments_daily.head())\n\n# Show summary statistics\nprint(\"\\nSummary by branch:\")\nprint(payments_daily.groupby(\"sucursal\")[\"ingreso_total\"].sum())\n</code></pre> <p>Key Features: - Uses <code>fetch_daily()</code> which automatically handles all ETL stages - Idempotent: safe to run multiple times - Returns daily aggregations ready for analysis</p>"},{"location":"user-guide/examples/#example-2-sales-core-fact","title":"Example 2: Sales Core Fact","text":"<p>File: <code>examples/sales_week_by_group.py</code></p> <p>Demonstrates fetching sales data at different aggregation levels:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.sales import core as sales_core\nfrom pos_core.sales import marts as sales_marts\n\n# Configure paths\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Get core fact (item-line grain)\nsales_items = sales_core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\nprint(f\"Core fact: {len(sales_items)} item lines\")\n\n# Get ticket-level mart\nsales_tickets = sales_marts.fetch_ticket(paths, \"2025-01-01\", \"2025-01-31\")\nprint(f\"Ticket mart: {len(sales_tickets)} tickets\")\n\n# Get group-level mart (pivot table)\nsales_groups = sales_marts.fetch_group(paths, \"2025-01-01\", \"2025-01-31\")\nprint(f\"Group mart: {len(sales_groups)} rows\")\nprint(sales_groups.head())\n</code></pre> <p>Key Features: - Shows different aggregation levels available - Demonstrates the relationship between core fact and marts - Group mart creates a pivot table by category</p>"},{"location":"user-guide/examples/#example-3-forecasting","title":"Example 3: Forecasting","text":"<p>File: <code>examples/payments_forecast.py</code></p> <p>Demonstrates generating forecasts for payment metrics:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import marts as payments_marts\nfrom pos_core.forecasting import ForecastConfig, run_payments_forecast\n\n# Configure paths\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Get historical data (need sufficient history for forecasting)\n# ARIMA models require at least 30 days\npayments_df = payments_marts.fetch_daily(\n    paths, \n    \"2022-01-01\", \n    \"2025-01-31\"\n)\n\n# Configure forecast\nconfig = ForecastConfig(horizon_days=91)  # 13 weeks\n\n# Run forecast\nresult = run_payments_forecast(payments_df, config)\n\n# Access results\nprint(\"Forecast Results:\")\nprint(result.forecast.head(20))\n\nprint(\"\\nDeposit Schedule:\")\nprint(result.deposit_schedule.head())\n\nprint(\"\\nMetadata:\")\nprint(f\"Branches: {result.metadata['branches']}\")\nprint(f\"Metrics: {result.metadata['metrics']}\")\nprint(f\"Horizon: {result.metadata['horizon_days']} days\")\n</code></pre> <p>Key Features: - Requires sufficient historical data (30+ days recommended) - Generates forecasts per branch and per metric - Includes deposit schedule for cash flow planning</p>"},{"location":"user-guide/examples/#with-debug-information","title":"With Debug Information","text":"<p>For advanced debugging, enable debug mode:</p> <pre><code># Run forecast with debug information\nresult = run_payments_forecast(payments_df, config, debug=True)\n\n# Access debug info for a specific model/branch/metric\nif result.debug:\n    naive_debug = result.debug[\"naive_last_week\"][\"Kavia\"][\"ingreso_efectivo\"]\n    print(f\"Model: {naive_debug.model_name}\")\n    print(f\"Source dates: {naive_debug.data['source_dates']}\")\n</code></pre>"},{"location":"user-guide/examples/#example-4-quality-assurance","title":"Example 4: Quality Assurance","text":"<p>Demonstrates running QA checks on payments data:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import marts as payments_marts\nfrom pos_core.qa import run_payments_qa\n\n# Configure paths\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Get payments data\ndf = payments_marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Run QA checks\nresult = run_payments_qa(df, level=4)\n\n# Print summary\nprint(\"QA Summary:\")\nprint(f\"Total rows: {result.summary['total_rows']}\")\nprint(f\"Total branches: {result.summary['total_sucursales']}\")\nprint(f\"Date range: {result.summary['min_fecha']} to {result.summary['max_fecha']}\")\nprint(f\"Missing days: {result.summary['missing_days_count']}\")\nprint(f\"Duplicates: {result.summary['duplicate_days_count']}\")\nprint(f\"Anomalies: {result.summary['zscore_anomalies_count']}\")\n\n# Print detailed findings\nif result.missing_days is not None:\n    print(\"\\nMissing Days:\")\n    print(result.missing_days)\n\nif result.zscore_anomalies is not None:\n    print(\"\\nZ-Score Anomalies:\")\n    print(result.zscore_anomalies.head())\n</code></pre> <p>Key Features: - Multiple QA levels available (0-4) - Detects missing days, duplicates, and statistical anomalies - Provides both summary and detailed findings</p>"},{"location":"user-guide/examples/#example-5-filtering-by-branch","title":"Example 5: Filtering by Branch","text":"<p>Demonstrates processing specific branches:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import marts as payments_marts\n\n# Configure paths\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Process only specific branches\nbranches = [\"Banana\", \"Queen\"]\n\npayments_df = payments_marts.fetch_daily(\n    paths,\n    \"2025-01-01\",\n    \"2025-01-31\",\n    branches=branches\n)\n\nprint(f\"Processed {len(branches)} branches\")\nprint(f\"Retrieved {len(payments_df)} rows\")\nprint(payments_df[\"sucursal\"].unique())\n</code></pre>"},{"location":"user-guide/examples/#example-6-force-refresh","title":"Example 6: Force Refresh","text":"<p>Demonstrates forcing a refresh of all ETL stages:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import marts as payments_marts\n\n# Configure paths\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Force refresh all stages (useful after fixing bugs or refreshing from source)\npayments_df = payments_marts.fetch_daily(\n    paths,\n    \"2025-01-01\",\n    \"2025-01-31\",\n    mode=\"force\"  # Force re-run all stages\n)\n\nprint(\"Data refreshed successfully\")\n</code></pre> <p>Use Cases: - After fixing a bug in transformation logic - When you want to refresh data from source - When debugging ETL issues</p>"},{"location":"user-guide/examples/#example-7-load-vs-fetch","title":"Example 7: Load vs Fetch","text":"<p>Demonstrates the difference between <code>fetch()</code> and <code>load()</code>:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import marts as payments_marts\n\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Fetch: ensures data exists, runs ETL if needed\ndf1 = payments_marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Load: read only (faster, but requires data to exist)\n# Raises FileNotFoundError if data doesn't exist\ntry:\n    df2 = payments_marts.load_daily(paths, \"2025-01-01\", \"2025-01-31\")\n    print(\"Loaded existing data successfully\")\nexcept FileNotFoundError:\n    print(\"Data doesn't exist, use fetch() instead\")\n</code></pre> <p>When to use: - <code>fetch()</code>: When you're not sure if data exists, or want automatic ETL - <code>load()</code>: When you're certain data exists and want faster reads</p>"},{"location":"user-guide/examples/#example-8-complete-workflow","title":"Example 8: Complete Workflow","text":"<p>Complete workflow combining ETL, forecasting, and QA:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import marts as payments_marts\nfrom pos_core.forecasting import ForecastConfig, run_payments_forecast\nfrom pos_core.qa import run_payments_qa\n\n# Configure paths\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Step 1: Get historical data\nprint(\"Step 1: Fetching payments data...\")\npayments_df = payments_marts.fetch_daily(\n    paths,\n    \"2022-01-01\",\n    \"2025-01-31\"\n)\nprint(f\"Retrieved {len(payments_df)} rows\")\n\n# Step 2: Run QA checks\nprint(\"\\nStep 2: Running QA checks...\")\nqa_result = run_payments_qa(payments_df)\nprint(f\"Missing days: {qa_result.summary['missing_days_count']}\")\nprint(f\"Anomalies: {qa_result.summary['zscore_anomalies_count']}\")\n\n# Step 3: Generate forecast\nprint(\"\\nStep 3: Generating forecast...\")\nconfig = ForecastConfig(horizon_days=91)\nforecast_result = run_payments_forecast(payments_df, config)\nprint(f\"Generated forecast for {len(forecast_result.forecast)} rows\")\n\n# Step 4: Display results\nprint(\"\\nStep 4: Forecast Summary:\")\nprint(forecast_result.forecast.groupby(\"sucursal\")[\"valor\"].sum())\n</code></pre>"},{"location":"user-guide/examples/#advanced-using-raw-data-layer","title":"Advanced: Using Raw Data Layer","text":"<p>For fine-grained control, you can work with raw data directly:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import raw\n\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Download raw data only\nraw.fetch(paths, \"2025-01-01\", \"2025-01-31\", mode=\"force\")\n\n# Load raw Excel files\nraw_df = raw.load(paths, \"2025-01-01\", \"2025-01-31\")\nprint(f\"Loaded {len(raw_df)} raw rows\")\n</code></pre>"},{"location":"user-guide/examples/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Learn about branch configuration and environment variables</li> <li>Concepts - Understand data layers, grains, and API design</li> <li>API Reference - Detailed function documentation</li> <li>Quickstart - Get started in minutes</li> </ul>"},{"location":"user-guide/examples/#more-information","title":"More Information","text":"<p>For detailed information about each example script, see the examples README in the repository.</p>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>This guide covers installing POS Core ETL and setting up your development environment.</p>"},{"location":"user-guide/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.10 or higher</li> <li>Operating System: Windows, macOS, or Linux</li> </ul>"},{"location":"user-guide/installation/#production-installation","title":"Production Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install pos-core-etl\n</code></pre>"},{"location":"user-guide/installation/#development-installation","title":"Development Installation","text":""},{"location":"user-guide/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/ToxicFyre/pos-pipeline-core-etl.git\ncd pos-pipeline-core-etl\n</code></pre>"},{"location":"user-guide/installation/#2-install-with-development-dependencies","title":"2. Install with Development Dependencies","text":"<pre><code>pip install -e .[dev]\n</code></pre> <p>This installs the package in editable mode with all development dependencies including: - Testing tools (pytest) - Code quality tools (ruff, mypy) - Documentation tools (mkdocs)</p>"},{"location":"user-guide/installation/#using-conda-recommended","title":"Using Conda (Recommended)","text":"<p>If you're using Anaconda or Miniconda:</p> <pre><code># Create environment\nconda create -n pos-etl python=3.10\nconda activate pos-etl\n\n# Install package\npip install -e .[dev]\n</code></pre>"},{"location":"user-guide/installation/#verify-installation","title":"Verify Installation","text":"<p>Test that the package is installed correctly:</p> <pre><code>from pos_core import DataPaths\nfrom pos_core.payments import core, marts\nfrom pos_core.sales import core as sales_core\n\nprint(\"POS Core ETL installed successfully!\")\n</code></pre>"},{"location":"user-guide/installation/#dependencies","title":"Dependencies","text":"<p>The package requires the following dependencies (installed automatically):</p> <ul> <li>pandas &gt;= 1.3.0 - Data manipulation</li> <li>numpy &gt;= 1.20.0 - Numerical computing</li> <li>requests &gt;= 2.25.0 - HTTP client for data extraction</li> <li>beautifulsoup4 &gt;= 4.9.0 - HTML parsing</li> <li>statsmodels &gt;= 0.12.0 - Time series forecasting</li> <li>openpyxl &gt;= 3.0.0 - Excel file handling</li> </ul>"},{"location":"user-guide/installation/#next-steps","title":"Next Steps","text":"<p>After installation:</p> <ol> <li>Configure your environment - Set up branch configuration and credentials</li> <li>Try the quickstart - Run your first ETL pipeline</li> <li>Explore examples - See complete working examples</li> </ol>"},{"location":"user-guide/quickstart/","title":"Quickstart","text":"<p>Get started with POS Core ETL in minutes. This guide walks you through setting up and running your first ETL pipeline.</p>"},{"location":"user-guide/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ol> <li>\u2705 Installed POS Core ETL (see Installation)</li> <li>\u2705 Access to a Wansoft-style POS system (or existing raw data files)</li> <li>\u2705 Python 3.10+ installed</li> </ol>"},{"location":"user-guide/quickstart/#step-1-create-branch-configuration","title":"Step 1: Create Branch Configuration","text":"<p>Create <code>utils/sucursales.json</code> with your branch configuration:</p> <pre><code>{\n  \"Banana\": {\n    \"code\": \"8888\",\n    \"valid_from\": \"2024-02-21\",\n    \"valid_to\": null\n  },\n  \"Queen\": {\n    \"code\": \"6362\",\n    \"valid_from\": \"2024-01-01\",\n    \"valid_to\": null\n  }\n}\n</code></pre> <p>Key fields: - <code>code</code>: The POS system code for this branch - <code>valid_from</code>: Start date when this code became active (YYYY-MM-DD format) - <code>valid_to</code>: End date when this code became inactive (null = still active)</p> <p>See Configuration for detailed information.</p>"},{"location":"user-guide/quickstart/#step-2-set-environment-variables","title":"Step 2: Set Environment Variables","text":"<p>Required for downloading raw data (skip if you already have raw data files):</p> <pre><code>export WS_BASE=\"https://your-pos-instance.com\"\nexport WS_USER=\"your_username\"\nexport WS_PASS=\"your_password\"\n</code></pre> <p>Note: If you're working with already-downloaded files in <code>a_raw/</code>, these environment variables are not needed.</p>"},{"location":"user-guide/quickstart/#step-3-create-data-directory-structure","title":"Step 3: Create Data Directory Structure","text":"<p>The package expects a specific directory structure:</p> <pre><code>data/\n\u251c\u2500\u2500 a_raw/          # Bronze: Raw Wansoft exports (Excel files)\n\u2502   \u251c\u2500\u2500 payments/\n\u2502   \u251c\u2500\u2500 sales/\n\u2502   \u2514\u2500\u2500 transfers/\n\u251c\u2500\u2500 b_clean/        # Silver: Core facts at atomic grain (CSV files)\n\u2502   \u251c\u2500\u2500 payments/\n\u2502   \u251c\u2500\u2500 sales/\n\u2502   \u2514\u2500\u2500 transfers/\n\u2514\u2500\u2500 c_processed/    # Gold: Marts (aggregated tables)\n    \u251c\u2500\u2500 payments/\n    \u251c\u2500\u2500 sales/\n    \u2514\u2500\u2500 transfers/\n</code></pre> <p>Create these directories:</p> <pre><code>mkdir -p data/{a_raw,b_clean,c_processed}/{payments,sales,transfers}\n</code></pre>"},{"location":"user-guide/quickstart/#step-4-run-your-first-etl","title":"Step 4: Run Your First ETL","text":"<p>Create a Python script <code>quickstart.py</code>:</p> <pre><code>from pathlib import Path\nfrom pos_core import DataPaths\nfrom pos_core.payments import marts as payments_marts\nfrom pos_core.sales import core as sales_core\nfrom pos_core.transfers import marts as transfers_marts\n\n# Configure paths\npaths = DataPaths.from_root(Path(\"data\"), Path(\"utils/sucursales.json\"))\n\n# Payments: daily mart (most common use case)\nprint(\"Fetching payments daily mart...\")\npayments_daily = payments_marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\nprint(f\"Retrieved {len(payments_daily)} rows\")\nprint(payments_daily.head())\n\n# Sales: core item-line fact\nprint(\"\\nFetching sales core fact...\")\nsales_items = sales_core.fetch(paths, \"2025-01-01\", \"2025-01-31\")\nprint(f\"Retrieved {len(sales_items)} rows\")\nprint(sales_items.head())\n\n# Transfers: pivot mart (branch \u00d7 category aggregation)\nprint(\"\\nFetching transfers pivot mart...\")\ntransfers_pivot = transfers_marts.fetch_pivot(paths, \"2025-01-01\", \"2025-01-31\")\nprint(f\"Retrieved pivot table with shape {transfers_pivot.shape}\")\nprint(transfers_pivot)\n</code></pre> <p>Run it:</p> <pre><code>python quickstart.py\n</code></pre>"},{"location":"user-guide/quickstart/#step-5-generate-a-forecast","title":"Step 5: Generate a Forecast","text":"<p>Add forecasting to your script:</p> <pre><code>from pos_core.forecasting import ForecastConfig, run_payments_forecast\n\n# Get historical data (need more data for forecasting)\npayments_df = payments_marts.fetch_daily(paths, \"2022-01-01\", \"2025-01-31\")\n\n# Run forecast\nconfig = ForecastConfig(horizon_days=91)  # 13 weeks\nresult = run_payments_forecast(payments_df, config)\n\nprint(\"\\nForecast Results:\")\nprint(result.forecast.head())\n\nprint(\"\\nDeposit Schedule:\")\nprint(result.deposit_schedule.head())\n</code></pre>"},{"location":"user-guide/quickstart/#step-6-run-quality-assurance","title":"Step 6: Run Quality Assurance","text":"<p>Add QA checks:</p> <pre><code>from pos_core.qa import run_payments_qa\n\ndf = payments_marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\nresult = run_payments_qa(df)\n\nprint(\"\\nQA Summary:\")\nprint(f\"Total rows: {result.summary['total_rows']}\")\nprint(f\"Missing days: {result.summary['missing_days_count']}\")\nprint(f\"Anomalies: {result.summary['zscore_anomalies_count']}\")\n\nif result.missing_days is not None:\n    print(\"\\nMissing Days:\")\n    print(result.missing_days)\n</code></pre>"},{"location":"user-guide/quickstart/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"user-guide/quickstart/#payments-daily-mart","title":"Payments Daily Mart","text":"<p>The <code>payments.marts.fetch_daily()</code> function returns a DataFrame with: - <code>sucursal</code>: Branch name - <code>fecha</code>: Date - <code>ingreso_efectivo</code>: Cash income - <code>ingreso_credito</code>: Credit card income - <code>ingreso_debito</code>: Debit card income - <code>num_tickets</code>: Number of tickets - Additional payment method columns</p>"},{"location":"user-guide/quickstart/#sales-core-fact","title":"Sales Core Fact","text":"<p>The <code>sales.core.fetch()</code> function returns a DataFrame with: - <code>sucursal</code>: Branch name - <code>operating_date</code>: Date of operation - <code>order_id</code>: Ticket/order identifier - <code>item_key</code>: Item identifier - <code>group</code>: Product group/category - <code>subtotal_item</code>: Item subtotal - <code>total_item</code>: Item total</p>"},{"location":"user-guide/quickstart/#transfers-pivot-mart","title":"Transfers Pivot Mart","text":"<p>The <code>transfers.marts.fetch_pivot()</code> function returns a pivot table with: - Rows: Branch codes (K, N, C, Q, PV, HZ, CC, TOTAL) - Columns: Product categories (NO-PROC, REFRICONGE, TOSTADOR, COMIDA SALADA, REPO, PAN DULCE Y SALADA, TOTAL)</p> <p>This aggregates transfer costs from CEDIS warehouse to retail branches by product category.</p>"},{"location":"user-guide/quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"user-guide/quickstart/#fetch-vs-load","title":"Fetch vs Load","text":"<ul> <li><code>fetch()</code>: Ensures data exists, runs ETL if needed</li> <li><code>load()</code>: Reads existing data only (raises error if missing)</li> </ul> <pre><code># Fetch: runs ETL if needed\ndf = payments_marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\")\n\n# Load: read only (faster, but requires existing data)\ndf = payments_marts.load_daily(paths, \"2025-01-01\", \"2025-01-31\")\n</code></pre>"},{"location":"user-guide/quickstart/#force-refresh","title":"Force Refresh","text":"<p>Force re-run ETL stages:</p> <pre><code># Force re-run all stages\ndf = payments_marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\", mode=\"force\")\n</code></pre>"},{"location":"user-guide/quickstart/#filter-by-branch","title":"Filter by Branch","text":"<p>Process specific branches:</p> <pre><code>df = payments_marts.fetch_daily(\n    paths, \n    \"2025-01-01\", \n    \"2025-01-31\",\n    branches=[\"Banana\", \"Queen\"]\n)\n</code></pre>"},{"location":"user-guide/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Concepts - Understand data layers, grains, and API design</li> <li>See Examples - Complete runnable example scripts</li> <li>Read API Reference - Detailed function documentation</li> <li>Configure Advanced Settings - Branch codes, date ranges, and more</li> </ul>"},{"location":"user-guide/quickstart/#troubleshooting","title":"Troubleshooting","text":"<p>Authentication Errors: Verify environment variables are set correctly: <pre><code>echo $WS_BASE\necho $WS_USER\necho $WS_PASS\n</code></pre></p> <p>Missing Data: Use <code>mode=\"force\"</code> to force re-extraction: <pre><code>df = payments_marts.fetch_daily(paths, \"2025-01-01\", \"2025-01-31\", mode=\"force\")\n</code></pre></p> <p>Insufficient Data for Forecasting: ARIMA models require at least 30 days of historical data.</p>"}]}